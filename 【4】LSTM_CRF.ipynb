{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab30dfb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:45.626758Z",
     "start_time": "2023-05-26T09:54:45.286818Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import eval4ner.muc as muc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a91ffaf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:47.223565Z",
     "start_time": "2023-05-26T09:54:45.628760Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel \n",
    "# torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b599170",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "465c4f98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:47.239565Z",
     "start_time": "2023-05-26T09:54:47.225569Z"
    }
   },
   "outputs": [],
   "source": [
    "filename=['中国图象图形学报','中文信息学报','模式识别与人工智能','计算机应用','计算机辅助设计与图形学学报']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1baad99d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:47.286565Z",
     "start_time": "2023-05-26T09:54:47.241566Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA=[]\n",
    "for f in filename:\n",
    "    with open(\"./DATA/\"+f+\".jsonl\",\"r+\",encoding=\"utf-8\") as ori:\n",
    "        for item in jsonlines.Reader(ori):\n",
    "            DATA.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8af5475",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:47.302564Z",
     "start_time": "2023-05-26T09:54:47.287567Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open('./title_all_.jsonl', 'w', encoding='utf-8') as f:\n",
    "#     for i in DATA:\n",
    "#         json.dump(i,f,ensure_ascii=False)\n",
    "#         f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b5e3983",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T13:51:06.423240Z",
     "start_time": "2023-07-23T13:51:06.419985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2501\n",
      "{'id': 32764, 'text': '3D多尺度深度卷积神经网络肺结节检测', 'label': [[5, 13, 'method'], [13, 18, 'RP'], [13, 18, 'rp']], 'Comments': []}\n",
      "[[5, 13, 'method'], [13, 18, 'RP'], [13, 18, 'rp']]\n"
     ]
    }
   ],
   "source": [
    "print(len(DATA))\n",
    "print(DATA[0])\n",
    "print(DATA[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "034440b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:47.333561Z",
     "start_time": "2023-05-26T09:54:47.319877Z"
    }
   },
   "outputs": [],
   "source": [
    "method=[]\n",
    "for i in range(len(DATA)):\n",
    "    for j in range(len(DATA[i]['label'])):\n",
    "        if DATA[i]['label'][j][2]=='rp':\n",
    "            m = DATA[i]['text'][int(DATA[i]['label'][j][0]):int(DATA[i]['label'][j][1])]\n",
    "            method.append(m)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b996677",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:47.365558Z",
     "start_time": "2023-05-26T09:54:47.335560Z"
    }
   },
   "outputs": [],
   "source": [
    "# method_df = pd.DataFrame({'methods':method}).to_csv('./method.csv',encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0a94665",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:47.381558Z",
     "start_time": "2023-05-26T09:54:47.367559Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "sen = [i['text'] for i in DATA]\n",
    "tags = ['O','B-MED','I-MED','B-rp','I-rp','<START>','<STOP>']\n",
    "\n",
    "for s in range(len(sen)):\n",
    "    l = ['O' for i in range(len(sen[s]))]\n",
    "    B_M=[]\n",
    "    E_M=[]\n",
    "    B_rp=[]\n",
    "    E_rp=[]\n",
    "    for label in DATA[s]['label']:\n",
    "        if label[2] == 'method':\n",
    "            l[label[0]] = 'B-MED'\n",
    "            l[label[0]+1:label[1]] = [\"I-MED\" for i in range(label[1]-label[0]-1)]\n",
    "        elif label[2] == 'rp':\n",
    "            l[label[0]] = 'B-rp'\n",
    "            l[label[0]+1:label[1]] = [\"I-rp\" for i in range(label[1]-label[0]-1)]\n",
    "            \n",
    "    L = \" \".join(l)\n",
    "    labels.append(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5791ec18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:47.617052Z",
     "start_time": "2023-05-26T09:54:47.607049Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "for i in range(len(sen)):\n",
    "    s=[]\n",
    "    for j in sen[i]:\n",
    "        s.append(j)\n",
    "    sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b8f0877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:47.760664Z",
     "start_time": "2023-05-26T09:54:47.754665Z"
    }
   },
   "outputs": [],
   "source": [
    "data=[]\n",
    "for i in range(len(sentences)):\n",
    "    data.append( (sentences[i], labels[i].split(' ')) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee640e91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:48.123478Z",
     "start_time": "2023-05-26T09:54:48.111479Z"
    }
   },
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "tag_to_ix = {}\n",
    "word_to_ix = {\"<UNK>\":0} #生字给id=0\n",
    "\n",
    "for sentence, tags in data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            \n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag]=len(tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ddf41f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:48.649527Z",
     "start_time": "2023-05-26T09:54:48.633544Z"
    }
   },
   "outputs": [],
   "source": [
    "tag_to_ix[START_TAG]=5\n",
    "tag_to_ix[STOP_TAG]=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7d50a88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:48.850891Z",
     "start_time": "2023-05-26T09:54:48.835893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-MED': 1,\n",
       " 'I-MED': 2,\n",
       " 'B-rp': 3,\n",
       " 'I-rp': 4,\n",
       " '<START>': 5,\n",
       " '<STOP>': 6}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e218899b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:50.781096Z",
     "start_time": "2023-05-26T09:54:49.018892Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_data,test_data=train_test_split(data,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a025e9",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bb2383f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:50.812096Z",
     "start_time": "2023-05-26T09:54:50.798098Z"
    },
    "code_folding": [
     13,
     19
    ]
   },
   "outputs": [],
   "source": [
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "# def prepare_sequence(seq, to_ix):\n",
    "#     idxs = [to_ix[w] for w in seq]\n",
    "#     return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "#这里增加未登录字处理\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] if w in to_ix else to_ix[\"<UNK>\"] for w in seq] #to_ix.has_key(x)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    # 计算当前步骤下的最优路径得分\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84ea7a03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:50.828096Z",
     "start_time": "2023-05-26T09:54:50.813096Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_method(output, text):\n",
    "    output = np.array(output)\n",
    "    m_b = np.where(output==1)\n",
    "    output[m_b]=2\n",
    "    m = np.where(output==2)\n",
    "    m = m[0].tolist()\n",
    "    m_text = []\n",
    "    if len(m)>0:\n",
    "        method=[[],[],[],[],[],[],[]]\n",
    "        method[0].append(m[0])\n",
    "        num=0\n",
    "        for i in range(1,len(m)):\n",
    "            if(m[i]-1==m[i-1]):\n",
    "                method[num].append(m[i])\n",
    "            else:\n",
    "                num+=1\n",
    "                method[num].append(m[i])\n",
    "        m_text = []\n",
    "        for i in method:\n",
    "            if len(i)!=0:\n",
    "                m_text.append((\"method\",text[i[0]:i[-1]+1]))\n",
    "    return m_text\n",
    "\n",
    "# predictions = get_method([0,0,1,2,2,2,0,1,2,2,2],'一个机器学习的智能算法')\n",
    "# print(predictions)\n",
    "# texts = '一个机器学习的智能算法'\n",
    "# muc.evaluate_one(predictions, predictions, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cef3c99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:50.842851Z",
     "start_time": "2023-05-26T09:54:50.829096Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_problem(output, text):\n",
    "    \n",
    "    output = np.array(output)\n",
    "    m_b = np.where(output==3)\n",
    "    output[m_b]=4\n",
    "    m = np.where(output==4)\n",
    "    m = m[0].tolist()\n",
    "#     print(m)\n",
    "    m_text = []\n",
    "    if len(m)>0:\n",
    "        method=[[],[],[],[],[],[],[]]\n",
    "        method[0].append(m[0])\n",
    "        num=0\n",
    "        for i in range(1,len(m)):\n",
    "            if(m[i]-1==m[i-1]):\n",
    "                \n",
    "                method[num].append(m[i])\n",
    "            else:\n",
    "                num+=1\n",
    "                method[num].append(m[i])\n",
    "        m_text = []\n",
    "        for i in method:\n",
    "            if len(i)!=0:\n",
    "                m_text.append((\"problem\",text[i[0]:i[-1]+1]))\n",
    "    return m_text\n",
    "\n",
    "# predictions = get_method([0,0,1,2,2,2,0,1,2,2,2],'一个机器学习的智能算法')\n",
    "# print(predictions)\n",
    "# texts = '一个机器学习的智能算法'\n",
    "# muc.evaluate_one(predictions, predictions, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5eef1d7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:51.605446Z",
     "start_time": "2023-05-26T09:54:51.600446Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def cal_M(model_,test_data):\n",
    "    with torch.no_grad():  \n",
    "        y_pred,y_true,texts=[],[],[]\n",
    "        for pair in test_data: #抽取部分看看效果\n",
    "            sentence=pair[0]\n",
    "            tag=pair[1]\n",
    "\n",
    "            precheck_sent = prepare_sequence(sentence, word_to_ix)\n",
    "            precheck_tags = torch.tensor([tag_to_ix[t] for t in tag], dtype=torch.long) \n",
    "            score,pred=model_(precheck_sent)\n",
    "            pred = torch.max(pred, 1)[1]\n",
    "            \n",
    "            sentence_ = \"\".join(sentence)\n",
    "\n",
    "            METHOD = get_method(pred,sentence_)\n",
    "            y_pred.append(METHOD)\n",
    "\n",
    "            method_ = get_method(precheck_tags,sentence_)\n",
    "            y_true.append(method_)\n",
    "            \n",
    "            texts.append(sentence_)\n",
    "#         print(y_pred)\n",
    "#         print(y_true)\n",
    "#         print(texts)\n",
    "        \n",
    "        return muc.evaluate_all(y_pred, y_true, texts)['exact']['f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4587317c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:51.947589Z",
     "start_time": "2023-05-26T09:54:51.934588Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def cal_P(model_,test_data):\n",
    "    with torch.no_grad():  \n",
    "        y_pred,y_true,texts=[],[],[]\n",
    "        for pair in test_data: #抽取部分看看效果\n",
    "            sentence=pair[0]\n",
    "            tag=pair[1]\n",
    "\n",
    "            precheck_sent = prepare_sequence(sentence, word_to_ix)\n",
    "            precheck_tags = torch.tensor([tag_to_ix[t] for t in tag], dtype=torch.long) \n",
    "            score,pred=model_(precheck_sent)\n",
    "            pred = torch.max(pred, 1)[1]\n",
    "            \n",
    "            sentence_ = \"\".join(sentence)\n",
    "\n",
    "            PROBLEM = get_problem(pred,sentence_)\n",
    "            y_pred.append(PROBLEM)\n",
    "\n",
    "            problem_ = get_problem(precheck_tags,sentence_)\n",
    "            y_true.append(problem_)\n",
    "            \n",
    "            texts.append(sentence_)\n",
    "#         print(y_pred)\n",
    "#         print(y_true)\n",
    "#         print(texts)\n",
    "        \n",
    "        return muc.evaluate_all(y_pred, y_true, texts)['exact']['f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "501e50f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:52.291076Z",
     "start_time": "2023-05-26T09:54:52.281076Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def cal_f1(model_,test_data):\n",
    "    with torch.no_grad():  \n",
    "        y_pred,y_true,texts=[],[],[]\n",
    "        for pair in test_data: #抽取部分看看效果\n",
    "            sentence=pair[0]\n",
    "            tag=pair[1]\n",
    "\n",
    "            precheck_sent = prepare_sequence(sentence, word_to_ix)\n",
    "            precheck_tags = torch.tensor([tag_to_ix[t] for t in tag], dtype=torch.long) \n",
    "            score,pred=model_(precheck_sent)\n",
    "\n",
    "            pred = torch.max(pred, 1)[1]\n",
    "            \n",
    "            sentence_ = \"\".join(sentence)\n",
    "\n",
    "            METHOD = get_method(pred,sentence_)\n",
    "            PROBLEM = get_problem(pred,sentence_)\n",
    "            METHOD.extend(PROBLEM)\n",
    "            y_pred.append(METHOD)\n",
    "\n",
    "            method_ = get_method(precheck_tags,sentence_)\n",
    "            problem_ = get_problem(precheck_tags,sentence_)\n",
    "            method_.extend(problem_)\n",
    "            y_true.append(method_)\n",
    "            texts.append(sentence_)\n",
    "#         print(y_pred)\n",
    "#         print(y_true)\n",
    "#         print(texts)\n",
    "        \n",
    "        return muc.evaluate_all(y_pred, y_true, texts)['exact']['f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e14ed1d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:52.880572Z",
     "start_time": "2023-05-26T09:54:52.862411Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def bert_cal_M(MODEl,test_data):\n",
    "    with torch.no_grad():  \n",
    "        y_pred,y_true,texts=[],[],[]\n",
    "        for pair in test_data: #抽取部分看看效果\n",
    "            sentence=pair[0]\n",
    "            tag=pair[1]\n",
    "\n",
    "        #         precheck_sent = prepare_sequence(sentence, word_to_ix)\n",
    "            input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0)\n",
    "            outputs = model(input_ids)\n",
    "            sequence_in = outputs[0][0][1:len(sentence)+1]\n",
    "            sequence_in = sequence_in.clone().detach().requires_grad_(True)\n",
    "\n",
    "\n",
    "            precheck_tags = torch.tensor([tag_to_ix[t] for t in tag], dtype=torch.long) \n",
    "            score,pred=MODEl(sequence_in)\n",
    "\n",
    "            sentence_ = \"\".join(sentence)\n",
    "\n",
    "            METHOD = get_method(pred,sentence_)\n",
    "            y_pred.append(METHOD)\n",
    "\n",
    "            method_ = get_method(precheck_tags,sentence_)\n",
    "            y_true.append(method_)\n",
    "\n",
    "            texts.append(sentence_)\n",
    "    #         print(y_pred)\n",
    "    #         print(y_true)\n",
    "    #         print(texts)\n",
    "\n",
    "    return muc.evaluate_all(y_pred, y_true, texts)['exact']['f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f790e1fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:53.161502Z",
     "start_time": "2023-05-26T09:54:53.147502Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    " def bert_cal_P(MODEl,test_data):\n",
    "    with torch.no_grad():  \n",
    "        y_pred,y_true,texts=[],[],[]\n",
    "        for pair in test_data: #抽取部分看看效果\n",
    "            sentence=pair[0]\n",
    "            tag=pair[1]\n",
    "\n",
    "        #         precheck_sent = prepare_sequence(sentence, word_to_ix)\n",
    "            input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0)\n",
    "            outputs = model(input_ids)\n",
    "            sequence_in = outputs[0][0][1:len(sentence)+1]\n",
    "            sequence_in = sequence_in.clone().detach().requires_grad_(True)\n",
    "\n",
    "\n",
    "            precheck_tags = torch.tensor([tag_to_ix[t] for t in tag], dtype=torch.long) \n",
    "            score,pred=MODEl(sequence_in)\n",
    "\n",
    "            sentence_ = \"\".join(sentence)\n",
    "            \n",
    "            PROBLEM = get_problem(pred,sentence_)\n",
    "            y_pred.append(PROBLEM)\n",
    "\n",
    "            problem_ = get_problem(precheck_tags,sentence_)\n",
    "            y_true.append(problem_)\n",
    "\n",
    "            texts.append(sentence_)\n",
    "    #         print(y_pred)\n",
    "    #         print(y_true)\n",
    "    #         print(texts)\n",
    "\n",
    "    return muc.evaluate_all(y_pred, y_true, texts)['exact']['f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64a1cc66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:53.471287Z",
     "start_time": "2023-05-26T09:54:53.451288Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def bert_cal_F1(MODEl,test_data):\n",
    "    with torch.no_grad():  \n",
    "        y_pred,y_true,texts=[],[],[]\n",
    "        for pair in test_data: #抽取部分看看效果\n",
    "            sentence=pair[0]\n",
    "            tag=pair[1]\n",
    "\n",
    "        #         precheck_sent = prepare_sequence(sentence, word_to_ix)\n",
    "            input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0)\n",
    "            outputs = model(input_ids)\n",
    "            sequence_in = outputs[0][0][1:len(sentence)+1]\n",
    "            sequence_in = sequence_in.clone().detach().requires_grad_(True)\n",
    "\n",
    "\n",
    "            precheck_tags = torch.tensor([tag_to_ix[t] for t in tag], dtype=torch.long) \n",
    "            score,pred=MODEl(sequence_in)\n",
    "\n",
    "            sentence_ = \"\".join(sentence)\n",
    "\n",
    "            METHOD = get_method(pred,sentence_)\n",
    "            PROBLEM = get_problem(pred,sentence_)\n",
    "            METHOD.extend(PROBLEM)\n",
    "            y_pred.append(METHOD)\n",
    "\n",
    "            method_ = get_method(precheck_tags,sentence_)\n",
    "            problem_ = get_problem(precheck_tags,sentence_)\n",
    "            method_.extend(problem_)\n",
    "            y_true.append(method_)\n",
    "\n",
    "            texts.append(sentence_)\n",
    "    #         print(y_pred)\n",
    "    #         print(y_true)\n",
    "    #         print(texts)\n",
    "\n",
    "    return muc.evaluate_all(y_pred, y_true, texts)['exact']['f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "667246c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:54.062499Z",
     "start_time": "2023-05-26T09:54:54.052500Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compare(y, y_pred):\n",
    "    error_index = []\n",
    "    if len(y) == len(y_pred):\n",
    "        for i in range(0, len(y)):\n",
    "            if y[i] != y_pred[i]:\n",
    "                error_index.append(i)\n",
    "\n",
    "    print(\"error_index:\",error_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "603c26fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:54.265647Z",
     "start_time": "2023-05-26T09:54:54.254646Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # 将LSTM的输出映射到标签空间\n",
    "        # 相当于公式中的发射矩阵U\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        \"\"\"初始化LSTM\"\"\"\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        \"\"\"预测函数，注意这个函数和_forward_alg不一样\n",
    "        这里给定一个句子，预测最有可能的标签序列\n",
    "        \"\"\"\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        \n",
    "        return _,lstm_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39848b12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:54.519042Z",
     "start_time": "2023-05-26T09:54:54.432044Z"
    },
    "code_folding": [
     30,
     35,
     69,
     80,
     84,
     90,
     136,
     144
    ]
   },
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # 将LSTM的输出映射到标签空间\n",
    "        # 相当于公式中的发射矩阵U\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # 转移矩阵，从标签i转移到标签j的分数\n",
    "        # tagset_size包含了人为加入的START_TAG和STOP_TAG\n",
    "        # transitions表示前一个字是j后一个是i的概率\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # 下面这两个约束不能转移到START_TAG，也不能从STOP_TAG开始转移\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        \"\"\"初始化LSTM\"\"\"\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        \"\"\"计算配分函数Z(x)\"\"\"\n",
    "\n",
    "        # 对应于伪码第一步\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # 对应于伪码第二步的循环，迭代整个句子\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                # 这里对应了伪码第二步中三者求和\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        # 对应于伪码第三步，注意损失函数最终是要logZ(x)，所以又是一个logsumexp\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        \"\"\"调用LSTM获得每个token的隐状态，这里可以替换为任意的特征函数，\n",
    "        LSTM返回的特征就是公式中的x\n",
    "        \"\"\"\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        \"\"\"计算给定输入序列和标签序列的匹配函数，即公式中的s函数\"\"\"\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        \"\"\"维特比解码，给定输入x和相关参数(发射矩阵和转移矩阵)，或者概率最大的标签序列\n",
    "        \"\"\"\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        \"\"\"损失函数 = Z(x) - s(x,y)\n",
    "        \"\"\"\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \"\"\"预测函数，注意这个函数和_forward_alg不一样\n",
    "        这里给定一个句子，预测最有可能的标签序列\n",
    "        \"\"\"\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b25e896",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:54.626551Z",
     "start_time": "2023-05-26T09:54:54.607551Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class BiLSTM_CRF_ATT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF_ATT, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "        self.Q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.K = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "\n",
    "        # 将LSTM的输出映射到标签空间\n",
    "        # 相当于公式中的发射矩阵U\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # 转移矩阵，从标签i转移到标签j的分数\n",
    "        # tagset_size包含了人为加入的START_TAG和STOP_TAG\n",
    "        # transitions表示前一个字是j后一个是i的概率\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # 下面这两个约束不能转移到START_TAG，也不能从STOP_TAG开始转移\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        \"\"\"初始化LSTM\"\"\"\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        \"\"\"计算配分函数Z(x)\"\"\"\n",
    "\n",
    "        # 对应于伪码第一步\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # 对应于伪码第二步的循环，迭代整个句子\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                # 这里对应了伪码第二步中三者求和\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        # 对应于伪码第三步，注意损失函数最终是要logZ(x)，所以又是一个logsumexp\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        \"\"\"调用LSTM获得每个token的隐状态，这里可以替换为任意的特征函数，\n",
    "        LSTM返回的特征就是公式中的x\n",
    "        \"\"\"\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        Q,K,V = lstm_out,lstm_out,lstm_out\n",
    "#         Q = self.Q(lstm_out)\n",
    "#         K = self.K(lstm_out)\n",
    "#         V = self.V(lstm_out)\n",
    "\n",
    "        scores = torch.matmul(Q,K.transpose(0,1))\n",
    "        alpha_n = F.softmax(scores,dim=-1)\n",
    "        context = torch.matmul(alpha_n,V)\n",
    "        \n",
    "        lstm_feats = self.hidden2tag(context)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        \"\"\"计算给定输入序列和标签序列的匹配函数，即公式中的s函数\"\"\"\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        \"\"\"维特比解码，给定输入x和相关参数(发射矩阵和转移矩阵)，或者概率最大的标签序列\n",
    "        \"\"\"\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        \"\"\"损失函数 = Z(x) - s(x,y)\n",
    "        \"\"\"\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \"\"\"预测函数，注意这个函数和_forward_alg不一样\n",
    "        这里给定一个句子，预测最有可能的标签序列\n",
    "        \"\"\"\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29af6484",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:54.965876Z",
     "start_time": "2023-05-26T09:54:54.936881Z"
    },
    "code_folding": [
     0,
     1,
     23,
     57,
     68,
     78,
     124
    ]
   },
   "outputs": [],
   "source": [
    "class Bert_CRF(nn.Module):\n",
    "    def __init__(self,tag_to_ix, embedding_dim):\n",
    "        super(Bert_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        #定义线性函数      \n",
    "        self.word_embeds = nn.Linear(768, self.embedding_dim)  #bert默认的隐藏单元数是768， 输出单元是2，表示二分类\n",
    "\n",
    "        # 将LSTM的输出映射到标签空间\n",
    "        # 相当于公式中的发射矩阵U\n",
    "        self.hidden2tag = nn.Linear(self.embedding_dim, self.tagset_size)\n",
    "\n",
    "        # 转移矩阵，从标签i转移到标签j的分数\n",
    "        # tagset_size包含了人为加入的START_TAG和STOP_TAG\n",
    "        # transitions表示前一个字是j后一个是i的概率\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # 下面这两个约束不能转移到START_TAG，也不能从STOP_TAG开始转移\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        \"\"\"计算配分函数Z(x)\"\"\"\n",
    "\n",
    "        # 对应于伪码第一步\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # 对应于伪码第二步的循环，迭代整个句子\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                # 这里对应了伪码第二步中三者求和\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        # 对应于伪码第三步，注意损失函数最终是要logZ(x)，所以又是一个logsumexp\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        \"\"\"调用LSTM获得每个token的隐状态，这里可以替换为任意的特征函数，\n",
    "        LSTM返回的特征就是公式中的x\n",
    "        \"\"\"\n",
    "\n",
    "        embeds = self.word_embeds(sentence).view(sentence.shape[0],1,-1).squeeze(1)\n",
    "        \n",
    "        lstm_feats = self.hidden2tag(embeds)\n",
    "        \n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        \"\"\"计算给定输入序列和标签序列的匹配函数，即公式中的s函数\"\"\"\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        \"\"\"维特比解码，给定输入x和相关参数(发射矩阵和转移矩阵)，或者概率最大的标签序列\n",
    "        \"\"\"\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, w2v, tags):\n",
    "        \"\"\"损失函数 = Z(x) - s(x,y)\n",
    "        \"\"\"\n",
    "        feats = self._get_lstm_features(w2v)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \"\"\"预测函数，注意这个函数和_forward_alg不一样\n",
    "        这里给定一个句子，预测最有可能的标签序列\n",
    "        \"\"\"\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35b14f8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:55.211544Z",
     "start_time": "2023-05-26T09:54:55.190193Z"
    },
    "code_folding": [
     0,
     24
    ]
   },
   "outputs": [],
   "source": [
    "class Bert_ATT_CRF(nn.Module):\n",
    "    def __init__(self,tag_to_ix, embedding_dim):\n",
    "        super(Bert_ATT_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        #定义线性函数      \n",
    "        self.word_embeds = nn.Linear(768, self.embedding_dim)  #bert默认的隐藏单元数是768， 输出单元是2，表示二分类\n",
    "\n",
    "        # 将LSTM的输出映射到标签空间\n",
    "        # 相当于公式中的发射矩阵U\n",
    "        self.hidden2tag = nn.Linear(self.embedding_dim, self.tagset_size)\n",
    "\n",
    "        # 转移矩阵，从标签i转移到标签j的分数\n",
    "        # tagset_size包含了人为加入的START_TAG和STOP_TAG\n",
    "        # transitions表示前一个字是j后一个是i的概率\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # 下面这两个约束不能转移到START_TAG，也不能从STOP_TAG开始转移\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        \"\"\"计算配分函数Z(x)\"\"\"\n",
    "\n",
    "        # 对应于伪码第一步\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # 对应于伪码第二步的循环，迭代整个句子\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                # 这里对应了伪码第二步中三者求和\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        # 对应于伪码第三步，注意损失函数最终是要logZ(x)，所以又是一个logsumexp\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        \"\"\"调用LSTM获得每个token的隐状态，这里可以替换为任意的特征函数，\n",
    "        LSTM返回的特征就是公式中的x\n",
    "        \"\"\"\n",
    "        embeds = self.word_embeds(sentence).view(sentence.shape[0],1,-1).squeeze(1)\n",
    "        \n",
    "        Q,K,V = embeds,embeds,embeds\n",
    "#         Q = self.Q(embeds)\n",
    "#         K = self.K(embeds)\n",
    "#         V = self.V(embeds)\n",
    "        scores = torch.matmul(Q,K.transpose(0,1))\n",
    "        alpha_n = F.softmax(scores,dim=-1)\n",
    "        context = torch.matmul(alpha_n,V)\n",
    "        \n",
    "        lstm_feats = self.hidden2tag(context)\n",
    "        \n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        \"\"\"计算给定输入序列和标签序列的匹配函数，即公式中的s函数\"\"\"\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        \"\"\"维特比解码，给定输入x和相关参数(发射矩阵和转移矩阵)，或者概率最大的标签序列\n",
    "        \"\"\"\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, w2v, tags):\n",
    "        \"\"\"损失函数 = Z(x) - s(x,y)\n",
    "        \"\"\"\n",
    "        feats = self._get_lstm_features(w2v)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \"\"\"预测函数，注意这个函数和_forward_alg不一样\n",
    "        这里给定一个句子，预测最有可能的标签序列\n",
    "        \"\"\"\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        \n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62e6935b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:55.634589Z",
     "start_time": "2023-05-26T09:54:55.618591Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Bert_BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self,tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(Bert_BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "#         self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        #定义线性函数      \n",
    "        self.word_embeds = nn.Linear(768, self.embedding_dim)  #bert默认的隐藏单元数是768， 输出单元是2，表示二分类\n",
    "        \n",
    "#         self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # 将LSTM的输出映射到标签空间\n",
    "        # 相当于公式中的发射矩阵U\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim, self.tagset_size)\n",
    "\n",
    "        # 转移矩阵，从标签i转移到标签j的分数\n",
    "        # tagset_size包含了人为加入的START_TAG和STOP_TAG\n",
    "        # transitions表示前一个字是j后一个是i的概率\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # 下面这两个约束不能转移到START_TAG，也不能从STOP_TAG开始转移\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        \"\"\"初始化LSTM\"\"\"\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        \"\"\"计算配分函数Z(x)\"\"\"\n",
    "\n",
    "        # 对应于伪码第一步\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # 对应于伪码第二步的循环，迭代整个句子\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                # 这里对应了伪码第二步中三者求和\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        # 对应于伪码第三步，注意损失函数最终是要logZ(x)，所以又是一个logsumexp\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        \"\"\"调用LSTM获得每个token的隐状态，这里可以替换为任意的特征函数，\n",
    "        LSTM返回的特征就是公式中的x\n",
    "        \"\"\"\n",
    "        \n",
    "#         self.hidden = self.init_hidden()\n",
    "#         embeds = self.word_embeds(sentence).view(sentence.shape[0],1,-1).squeeze(1)\n",
    "#         Q,K,V = embeds,embeds,embeds\n",
    "#         Q = self.Q(lstm_out)\n",
    "#         K = self.K(lstm_out)\n",
    "#         V = self.V(lstm_out)\n",
    "#         scores = torch.matmul(Q,K.transpose(0,1))\n",
    "#         alpha_n = F.softmax(scores,dim=-1)\n",
    "#         context = torch.matmul(alpha_n,V)\n",
    "#         lstm_feats = self.hidden2tag(w2v)\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        \n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        \"\"\"计算给定输入序列和标签序列的匹配函数，即公式中的s函数\"\"\"\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        \"\"\"维特比解码，给定输入x和相关参数(发射矩阵和转移矩阵)，或者概率最大的标签序列\n",
    "        \"\"\"\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, w2v, tags):\n",
    "        \"\"\"损失函数 = Z(x) - s(x,y)\n",
    "        \"\"\"\n",
    "        feats = self._get_lstm_features(w2v)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \"\"\"预测函数，注意这个函数和_forward_alg不一样\n",
    "        这里给定一个句子，预测最有可能的标签序列\n",
    "        \"\"\"\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888c26c",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0487978",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:57.221566Z",
     "start_time": "2023-05-26T09:54:57.213567Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train_lstm(model,optimizer,EPOCHS):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    F1=[]\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(\"epoch %d =============\" % epoch)\n",
    "        time_start = time.time()\n",
    "\n",
    "        count=1\n",
    "        for sentence, tags in training_data:\n",
    "            model.zero_grad()\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix) \n",
    "            targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            _,pred = model(sentence_in)\n",
    "            loss = criterion(pred,targets)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            # calling optimizer.step()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if count%500==0:\n",
    "                print(\"iter %d: loss %f\" %(count,loss))\n",
    "            count+=1\n",
    "\n",
    "        time_end=time.time()\n",
    "        F1.append(cal_f1(model,test_data))\n",
    "        print(\"time used: %d s\" % (time_end-time_start)  )\n",
    "        \n",
    "    return F1,model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1cb7712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:57.519668Z",
     "start_time": "2023-05-26T09:54:57.508668Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train(model,optimizer,EPOCHS):\n",
    "\n",
    "    F1=[]\n",
    "    for epoch in range(EPOCHS):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "        print(\"epoch %d =============\" % epoch)\n",
    "        time_start = time.time()\n",
    "\n",
    "        count=1\n",
    "        for sentence, tags in training_data:\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is,\n",
    "            # turn them into Tensors of word indices.\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix) \n",
    "            targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            # calling optimizer.step()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if count==500 or count==1000:\n",
    "                print(\"iter %d: loss %f\" %(count,loss))\n",
    "            count+=1\n",
    "\n",
    "        time_end=time.time()\n",
    "        F1.append(cal_f1(model,test_data))\n",
    "        print(\"time used: %d s\" % (time_end-time_start)  )\n",
    "        \n",
    "    return F1,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54e1295c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:57.899705Z",
     "start_time": "2023-05-26T09:54:57.880706Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_bert(bert_model,optimizer,EPOCHS):\n",
    "    F1=[]\n",
    "\n",
    "    for epoch in range(EPOCHS):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "        print(\"epoch %d =============\" % epoch)\n",
    "        time_start = time.time()\n",
    "\n",
    "        count=1\n",
    "        for sentence, tags in training_data:\n",
    "            \n",
    "            bert_model.zero_grad()\n",
    "            \n",
    "          # 使用这种方法对句子编码会自动添加[CLS] 和[SEP]\n",
    "            input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0)\n",
    "            outputs = model(input_ids)\n",
    "            sequence_in = outputs[0][0][1:len(sentence)+1]\n",
    "            sequence_in = sequence_in.clone().detach().requires_grad_(True)\n",
    "\n",
    "            targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            loss = bert_model.neg_log_likelihood(sequence_in, targets)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            # calling optimizer.step()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if count==500 or count==1000:\n",
    "#                 score,y_pred=bert_model(sequence_in)\n",
    "    #             print(sentence)\n",
    "    #             print(targets)\n",
    "    #             print(y_pred)\n",
    "                print(\"iter %d: loss %f\" %(count,loss))\n",
    "            count+=1\n",
    "\n",
    "        time_end=time.time()\n",
    "        F1.append(bert_cal_F1(bert_model,test_data))\n",
    "        print(\"time used: %d s\" % (time_end-time_start))\n",
    "        \n",
    "    return F1,bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "642878a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:54:59.742362Z",
     "start_time": "2023-05-26T09:54:59.730364Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 64*2\n",
    "EPOCHS=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8e265426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T16:22:26.871548Z",
     "start_time": "2023-05-11T16:19:44.044027Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 =============\n",
      "iter 500: loss 0.757516\n",
      "iter 1000: loss 0.278210\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6262, Recall=0.6530, F1:0.6356\n",
      "   exact mode, Precision=0.6279, Recall=0.6547, F1:0.6373\n",
      " partial mode, Precision=0.7794, Recall=0.8221, F1:0.7945\n",
      "    type mode, Precision=0.9206, Recall=0.9745, F1:0.9400\n",
      "time used: 10 s\n",
      "epoch 1 =============\n",
      "iter 500: loss 0.428748\n",
      "iter 1000: loss 0.170184\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6529, Recall=0.6823, F1:0.6629\n",
      "   exact mode, Precision=0.6538, Recall=0.6834, F1:0.6638\n",
      " partial mode, Precision=0.7958, Recall=0.8365, F1:0.8097\n",
      "    type mode, Precision=0.9260, Recall=0.9729, F1:0.9419\n",
      "time used: 10 s\n",
      "epoch 2 =============\n",
      "iter 500: loss 0.322457\n",
      "iter 1000: loss 0.136235\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6719, Recall=0.6987, F1:0.6811\n",
      "   exact mode, Precision=0.6727, Recall=0.6998, F1:0.6820\n",
      " partial mode, Precision=0.8081, Recall=0.8455, F1:0.8211\n",
      "    type mode, Precision=0.9337, Recall=0.9773, F1:0.9490\n",
      "time used: 10 s\n",
      "epoch 3 =============\n",
      "iter 500: loss 0.260165\n",
      "iter 1000: loss 0.125469\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6708, Recall=0.6967, F1:0.6801\n",
      "   exact mode, Precision=0.6733, Recall=0.6995, F1:0.6827\n",
      " partial mode, Precision=0.8103, Recall=0.8456, F1:0.8230\n",
      "    type mode, Precision=0.9370, Recall=0.9779, F1:0.9517\n",
      "time used: 10 s\n",
      "epoch 4 =============\n",
      "iter 500: loss 0.258896\n",
      "iter 1000: loss 0.059059\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6834, Recall=0.7089, F1:0.6922\n",
      "   exact mode, Precision=0.6867, Recall=0.7122, F1:0.6956\n",
      " partial mode, Precision=0.8174, Recall=0.8517, F1:0.8294\n",
      "    type mode, Precision=0.9374, Recall=0.9773, F1:0.9514\n",
      "time used: 10 s\n",
      "epoch 5 =============\n",
      "iter 500: loss 0.219041\n",
      "iter 1000: loss 0.081831\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6808, Recall=0.7084, F1:0.6903\n",
      "   exact mode, Precision=0.6824, Recall=0.7100, F1:0.6920\n",
      " partial mode, Precision=0.8121, Recall=0.8500, F1:0.8255\n",
      "    type mode, Precision=0.9304, Recall=0.9751, F1:0.9463\n",
      "time used: 11 s\n",
      "epoch 6 =============\n",
      "iter 500: loss 0.236362\n",
      "iter 1000: loss 0.052065\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6828, Recall=0.7056, F1:0.6898\n",
      "   exact mode, Precision=0.6844, Recall=0.7073, F1:0.6914\n",
      " partial mode, Precision=0.8149, Recall=0.8464, F1:0.8247\n",
      "    type mode, Precision=0.9375, Recall=0.9751, F1:0.9491\n",
      "time used: 10 s\n",
      "epoch 7 =============\n",
      "iter 500: loss 0.208048\n",
      "iter 1000: loss 0.041900\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7069, Recall=0.7308, F1:0.7145\n",
      "   exact mode, Precision=0.7085, Recall=0.7324, F1:0.7162\n",
      " partial mode, Precision=0.8299, Recall=0.8604, F1:0.8399\n",
      "    type mode, Precision=0.9419, Recall=0.9773, F1:0.9536\n",
      "time used: 11 s\n",
      "epoch 8 =============\n",
      "iter 500: loss 0.168665\n",
      "iter 1000: loss 0.038341\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7052, Recall=0.7233, F1:0.7099\n",
      "   exact mode, Precision=0.7069, Recall=0.7250, F1:0.7116\n",
      " partial mode, Precision=0.8299, Recall=0.8558, F1:0.8375\n",
      "    type mode, Precision=0.9428, Recall=0.9740, F1:0.9523\n",
      "time used: 10 s\n",
      "epoch 9 =============\n",
      "iter 500: loss 0.174245\n",
      "iter 1000: loss 0.034134\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7007, Recall=0.7233, F1:0.7078\n",
      "   exact mode, Precision=0.7024, Recall=0.7250, F1:0.7095\n",
      " partial mode, Precision=0.8261, Recall=0.8561, F1:0.8357\n",
      "    type mode, Precision=0.9401, Recall=0.9767, F1:0.9520\n",
      "time used: 10 s\n",
      "epoch 10 =============\n",
      "iter 500: loss 0.232200\n",
      "iter 1000: loss 0.033991\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6990, Recall=0.7156, F1:0.7033\n",
      "   exact mode, Precision=0.7007, Recall=0.7172, F1:0.7050\n",
      " partial mode, Precision=0.8267, Recall=0.8503, F1:0.8333\n",
      "    type mode, Precision=0.9446, Recall=0.9729, F1:0.9525\n",
      "time used: 10 s\n",
      "epoch 11 =============\n",
      "iter 500: loss 0.192964\n",
      "iter 1000: loss 0.030063\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7003, Recall=0.7255, F1:0.7088\n",
      "   exact mode, Precision=0.7020, Recall=0.7272, F1:0.7104\n",
      " partial mode, Precision=0.8224, Recall=0.8569, F1:0.8342\n",
      "    type mode, Precision=0.9309, Recall=0.9729, F1:0.9453\n",
      "time used: 10 s\n",
      "epoch 12 =============\n",
      "iter 500: loss 0.156568\n",
      "iter 1000: loss 0.024171\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6973, Recall=0.7175, F1:0.7033\n",
      "   exact mode, Precision=0.6990, Recall=0.7192, F1:0.7049\n",
      " partial mode, Precision=0.8242, Recall=0.8521, F1:0.8327\n",
      "    type mode, Precision=0.9374, Recall=0.9712, F1:0.9478\n",
      "time used: 10 s\n",
      "epoch 13 =============\n",
      "iter 500: loss 0.177834\n",
      "iter 1000: loss 0.027149\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7051, Recall=0.7264, F1:0.7117\n",
      "   exact mode, Precision=0.7068, Recall=0.7280, F1:0.7133\n",
      " partial mode, Precision=0.8263, Recall=0.8568, F1:0.8362\n",
      "    type mode, Precision=0.9339, Recall=0.9712, F1:0.9462\n",
      "time used: 9 s\n",
      "epoch 14 =============\n",
      "iter 500: loss 0.146404\n",
      "iter 1000: loss 0.018604\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6975, Recall=0.7208, F1:0.7050\n",
      "   exact mode, Precision=0.6992, Recall=0.7225, F1:0.7067\n",
      " partial mode, Precision=0.8214, Recall=0.8557, F1:0.8332\n",
      "    type mode, Precision=0.9354, Recall=0.9790, F1:0.9508\n",
      "time used: 9 s\n"
     ]
    }
   ],
   "source": [
    "bilstm = BiLSTM(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer0 = optim.Adagrad(bilstm.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "F1_bilstm,bilstm = train_lstm(bilstm,optimizer0,EPOCHS)\n",
    "torch.save(bilstm.state_dict(),'./Model/bilstm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f95d8ef1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T04:55:11.079998Z",
     "start_time": "2023-04-24T04:45:05.217425Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 =============\n",
      "iter 500: loss 17.308954\n",
      "iter 1000: loss 6.905617\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6130, Recall=0.6496, F1:0.6263\n",
      "   exact mode, Precision=0.6130, Recall=0.6496, F1:0.6263\n",
      " partial mode, Precision=0.7614, Recall=0.8198, F1:0.7825\n",
      "    type mode, Precision=0.9021, Recall=0.9784, F1:0.9294\n",
      "time used: 39 s\n",
      "epoch 1 =============\n",
      "iter 500: loss 9.706703\n",
      "iter 1000: loss 4.953720\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6709, Recall=0.7009, F1:0.6818\n",
      "   exact mode, Precision=0.6709, Recall=0.7009, F1:0.6818\n",
      " partial mode, Precision=0.8022, Recall=0.8455, F1:0.8177\n",
      "    type mode, Precision=0.9289, Recall=0.9823, F1:0.9478\n",
      "time used: 39 s\n",
      "epoch 2 =============\n",
      "iter 500: loss 6.836761\n",
      "iter 1000: loss 3.642677\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7019, Recall=0.7302, F1:0.7124\n",
      "   exact mode, Precision=0.7036, Recall=0.7319, F1:0.7140\n",
      " partial mode, Precision=0.8221, Recall=0.8615, F1:0.8364\n",
      "    type mode, Precision=0.9349, Recall=0.9834, F1:0.9523\n",
      "time used: 38 s\n",
      "epoch 3 =============\n",
      "iter 500: loss 7.476097\n",
      "iter 1000: loss 2.480576\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6992, Recall=0.7288, F1:0.7102\n",
      "   exact mode, Precision=0.6992, Recall=0.7288, F1:0.7102\n",
      " partial mode, Precision=0.8192, Recall=0.8567, F1:0.8330\n",
      "    type mode, Precision=0.9360, Recall=0.9801, F1:0.9521\n",
      "time used: 38 s\n",
      "epoch 4 =============\n",
      "iter 500: loss 6.927723\n",
      "iter 1000: loss 2.067139\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7174, Recall=0.7410, F1:0.7261\n",
      "   exact mode, Precision=0.7191, Recall=0.7427, F1:0.7278\n",
      " partial mode, Precision=0.8355, Recall=0.8655, F1:0.8462\n",
      "    type mode, Precision=0.9479, Recall=0.9839, F1:0.9604\n",
      "time used: 38 s\n",
      "epoch 5 =============\n",
      "iter 500: loss 5.976662\n",
      "iter 1000: loss 1.537285\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7240, Recall=0.7471, F1:0.7327\n",
      "   exact mode, Precision=0.7257, Recall=0.7488, F1:0.7344\n",
      " partial mode, Precision=0.8401, Recall=0.8686, F1:0.8504\n",
      "    type mode, Precision=0.9489, Recall=0.9823, F1:0.9607\n",
      "time used: 41 s\n",
      "epoch 6 =============\n",
      "iter 500: loss 5.892811\n",
      "iter 1000: loss 1.289871\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7329, Recall=0.7499, F1:0.7387\n",
      "   exact mode, Precision=0.7346, Recall=0.7516, F1:0.7404\n",
      " partial mode, Precision=0.8479, Recall=0.8672, F1:0.8541\n",
      "    type mode, Precision=0.9562, Recall=0.9779, F1:0.9628\n",
      "time used: 40 s\n",
      "epoch 7 =============\n",
      "iter 500: loss 6.106789\n",
      "iter 1000: loss 0.876579\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7266, Recall=0.7444, F1:0.7328\n",
      "   exact mode, Precision=0.7266, Recall=0.7444, F1:0.7328\n",
      " partial mode, Precision=0.8416, Recall=0.8633, F1:0.8489\n",
      "    type mode, Precision=0.9540, Recall=0.9795, F1:0.9625\n",
      "time used: 39 s\n",
      "epoch 8 =============\n",
      "iter 500: loss 5.790459\n",
      "iter 1000: loss 0.736542\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7462, Recall=0.7582, F1:0.7491\n",
      "   exact mode, Precision=0.7462, Recall=0.7582, F1:0.7491\n",
      " partial mode, Precision=0.8555, Recall=0.8711, F1:0.8594\n",
      "    type mode, Precision=0.9615, Recall=0.9806, F1:0.9665\n",
      "time used: 39 s\n",
      "epoch 9 =============\n",
      "iter 500: loss 4.756393\n",
      "iter 1000: loss 0.599098\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7435, Recall=0.7576, F1:0.7477\n",
      "   exact mode, Precision=0.7452, Recall=0.7593, F1:0.7494\n",
      " partial mode, Precision=0.8557, Recall=0.8727, F1:0.8607\n",
      "    type mode, Precision=0.9604, Recall=0.9801, F1:0.9661\n",
      "time used: 40 s\n",
      "epoch 10 =============\n",
      "iter 500: loss 4.359528\n",
      "iter 1000: loss 0.392853\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7415, Recall=0.7532, F1:0.7440\n",
      "   exact mode, Precision=0.7432, Recall=0.7549, F1:0.7457\n",
      " partial mode, Precision=0.8536, Recall=0.8694, F1:0.8573\n",
      "    type mode, Precision=0.9599, Recall=0.9795, F1:0.9646\n",
      "time used: 39 s\n",
      "epoch 11 =============\n",
      "iter 500: loss 3.748367\n",
      "iter 1000: loss 0.348984\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7443, Recall=0.7488, F1:0.7434\n",
      "   exact mode, Precision=0.7460, Recall=0.7504, F1:0.7451\n",
      " partial mode, Precision=0.8562, Recall=0.8625, F1:0.8554\n",
      "    type mode, Precision=0.9615, Recall=0.9695, F1:0.9607\n",
      "time used: 37 s\n",
      "epoch 12 =============\n",
      "iter 500: loss 3.951347\n",
      "iter 1000: loss 0.344254\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7385, Recall=0.7510, F1:0.7414\n",
      "   exact mode, Precision=0.7402, Recall=0.7527, F1:0.7431\n",
      " partial mode, Precision=0.8518, Recall=0.8680, F1:0.8558\n",
      "    type mode, Precision=0.9601, Recall=0.9801, F1:0.9651\n",
      "time used: 38 s\n",
      "epoch 13 =============\n",
      "iter 500: loss 3.284119\n",
      "iter 1000: loss 0.289825\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7534, Recall=0.7599, F1:0.7532\n",
      "   exact mode, Precision=0.7550, Recall=0.7615, F1:0.7548\n",
      " partial mode, Precision=0.8623, Recall=0.8725, F1:0.8632\n",
      "    type mode, Precision=0.9637, Recall=0.9773, F1:0.9656\n",
      "time used: 36 s\n",
      "epoch 14 =============\n",
      "iter 500: loss 3.343552\n",
      "iter 1000: loss 0.234741\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7518, Recall=0.7527, F1:0.7489\n",
      "   exact mode, Precision=0.7535, Recall=0.7543, F1:0.7505\n",
      " partial mode, Precision=0.8607, Recall=0.8647, F1:0.8585\n",
      "    type mode, Precision=0.9621, Recall=0.9690, F1:0.9605\n",
      "time used: 37 s\n"
     ]
    }
   ],
   "source": [
    "bilstm_crf = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer1 = optim.Adagrad(bilstm_crf.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "F1_bilstm_crf,bilstm_crf = train(bilstm_crf,optimizer1,EPOCHS)\n",
    "torch.save(bilstm_crf.state_dict(),'./Model/bilstm_crf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5f89beed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T05:04:47.339021Z",
     "start_time": "2023-04-24T04:55:11.079998Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 =============\n",
      "iter 500: loss 10.339867\n",
      "iter 1000: loss 8.545563\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6342, Recall=0.6519, F1:0.6400\n",
      "   exact mode, Precision=0.6386, Recall=0.6564, F1:0.6444\n",
      " partial mode, Precision=0.7930, Recall=0.8145, F1:0.7994\n",
      "    type mode, Precision=0.9346, Recall=0.9576, F1:0.9407\n",
      "time used: 37 s\n",
      "epoch 1 =============\n",
      "iter 500: loss 4.933735\n",
      "iter 1000: loss 3.919632\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6728, Recall=0.6886, F1:0.6775\n",
      "   exact mode, Precision=0.6772, Recall=0.6930, F1:0.6819\n",
      " partial mode, Precision=0.8140, Recall=0.8323, F1:0.8191\n",
      "    type mode, Precision=0.9403, Recall=0.9599, F1:0.9453\n",
      "time used: 37 s\n",
      "epoch 2 =============\n",
      "iter 500: loss 2.256699\n",
      "iter 1000: loss 2.300110\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6992, Recall=0.7109, F1:0.7019\n",
      "   exact mode, Precision=0.7053, Recall=0.7169, F1:0.7079\n",
      " partial mode, Precision=0.8309, Recall=0.8446, F1:0.8340\n",
      "    type mode, Precision=0.9488, Recall=0.9640, F1:0.9521\n",
      "time used: 37 s\n",
      "epoch 3 =============\n",
      "iter 500: loss 1.905033\n",
      "iter 1000: loss 1.766853\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7213, Recall=0.7324, F1:0.7235\n",
      "   exact mode, Precision=0.7252, Recall=0.7369, F1:0.7276\n",
      " partial mode, Precision=0.8434, Recall=0.8560, F1:0.8457\n",
      "    type mode, Precision=0.9535, Recall=0.9662, F1:0.9555\n",
      "time used: 38 s\n",
      "epoch 4 =============\n",
      "iter 500: loss 1.607201\n",
      "iter 1000: loss 1.466431\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7139, Recall=0.7208, F1:0.7137\n",
      "   exact mode, Precision=0.7167, Recall=0.7236, F1:0.7164\n",
      " partial mode, Precision=0.8394, Recall=0.8457, F1:0.8382\n",
      "    type mode, Precision=0.9549, Recall=0.9607, F1:0.9528\n",
      "time used: 37 s\n",
      "epoch 5 =============\n",
      "iter 500: loss 1.391258\n",
      "iter 1000: loss 1.516640\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7272, Recall=0.7358, F1:0.7283\n",
      "   exact mode, Precision=0.7333, Recall=0.7419, F1:0.7344\n",
      " partial mode, Precision=0.8500, Recall=0.8585, F1:0.8506\n",
      "    type mode, Precision=0.9599, Recall=0.9679, F1:0.9599\n",
      "time used: 37 s\n",
      "epoch 6 =============\n",
      "iter 500: loss 1.472630\n",
      "iter 1000: loss 1.013695\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7244, Recall=0.7324, F1:0.7254\n",
      "   exact mode, Precision=0.7300, Recall=0.7380, F1:0.7309\n",
      " partial mode, Precision=0.8482, Recall=0.8560, F1:0.8484\n",
      "    type mode, Precision=0.9601, Recall=0.9673, F1:0.9594\n",
      "time used: 37 s\n",
      "epoch 7 =============\n",
      "iter 500: loss 1.391800\n",
      "iter 1000: loss 1.035446\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7319, Recall=0.7341, F1:0.7295\n",
      "   exact mode, Precision=0.7380, Recall=0.7402, F1:0.7356\n",
      " partial mode, Precision=0.8536, Recall=0.8554, F1:0.8504\n",
      "    type mode, Precision=0.9623, Recall=0.9635, F1:0.9582\n",
      "time used: 37 s\n",
      "epoch 8 =============\n",
      "iter 500: loss 1.006527\n",
      "iter 1000: loss 1.000595\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7266, Recall=0.7230, F1:0.7216\n",
      "   exact mode, Precision=0.7327, Recall=0.7291, F1:0.7277\n",
      " partial mode, Precision=0.8533, Recall=0.8485, F1:0.8472\n",
      "    type mode, Precision=0.9679, Recall=0.9618, F1:0.9606\n",
      "time used: 36 s\n",
      "epoch 9 =============\n",
      "iter 500: loss 1.197624\n",
      "iter 1000: loss 0.592094\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7333, Recall=0.7374, F1:0.7323\n",
      "   exact mode, Precision=0.7360, Recall=0.7402, F1:0.7351\n",
      " partial mode, Precision=0.8521, Recall=0.8563, F1:0.8506\n",
      "    type mode, Precision=0.9621, Recall=0.9662, F1:0.9599\n",
      "time used: 36 s\n",
      "epoch 10 =============\n",
      "iter 500: loss 2.127251\n",
      "iter 1000: loss 0.622047\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7452, Recall=0.7502, F1:0.7445\n",
      "   exact mode, Precision=0.7496, Recall=0.7546, F1:0.7489\n",
      " partial mode, Precision=0.8594, Recall=0.8657, F1:0.8589\n",
      "    type mode, Precision=0.9621, Recall=0.9690, F1:0.9615\n",
      "time used: 36 s\n",
      "epoch 11 =============\n",
      "iter 500: loss 1.201759\n",
      "iter 1000: loss 0.443047\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7504, Recall=0.7535, F1:0.7490\n",
      "   exact mode, Precision=0.7549, Recall=0.7579, F1:0.7534\n",
      " partial mode, Precision=0.8643, Recall=0.8665, F1:0.8620\n",
      "    type mode, Precision=0.9668, Recall=0.9679, F1:0.9635\n",
      "time used: 36 s\n",
      "epoch 12 =============\n",
      "iter 500: loss 1.533806\n",
      "iter 1000: loss 0.420502\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7488, Recall=0.7496, F1:0.7458\n",
      "   exact mode, Precision=0.7532, Recall=0.7540, F1:0.7503\n",
      " partial mode, Precision=0.8626, Recall=0.8623, F1:0.8586\n",
      "    type mode, Precision=0.9648, Recall=0.9635, F1:0.9598\n",
      "time used: 36 s\n",
      "epoch 13 =============\n",
      "iter 500: loss 1.385780\n",
      "iter 1000: loss 0.274307\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7449, Recall=0.7441, F1:0.7410\n",
      "   exact mode, Precision=0.7504, Recall=0.7496, F1:0.7465\n",
      " partial mode, Precision=0.8622, Recall=0.8599, F1:0.8569\n",
      "    type mode, Precision=0.9668, Recall=0.9629, F1:0.9601\n",
      "time used: 36 s\n",
      "epoch 14 =============\n",
      "iter 500: loss 0.642776\n",
      "iter 1000: loss 0.226532\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7524, Recall=0.7540, F1:0.7499\n",
      "   exact mode, Precision=0.7568, Recall=0.7585, F1:0.7543\n",
      " partial mode, Precision=0.8653, Recall=0.8668, F1:0.8622\n",
      "    type mode, Precision=0.9676, Recall=0.9690, F1:0.9640\n",
      "time used: 36 s\n"
     ]
    }
   ],
   "source": [
    "bilstm_att_crf = BiLSTM_CRF_ATT(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer2 = optim.Adagrad(bilstm_att_crf.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "F1_bilstm_att_crf,bilstm_att_crf = train(bilstm_att_crf,optimizer2,EPOCHS)\n",
    "torch.save(bilstm_att_crf.state_dict(),'./Model/bilstm_att_crf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "486a90b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:55:03.870688Z",
     "start_time": "2023-05-26T09:55:02.937748Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../bert-base/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights=\"../bert-base/bert-base-chinese/\"\n",
    "model = BertModel.from_pretrained(pretrained_weights)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35c3042e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T06:52:51.180600Z",
     "start_time": "2023-04-24T06:16:57.442998Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 =============\n",
      "iter 500: loss 12.053375\n",
      "iter 1000: loss 2.533348\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6504, Recall=0.6924, F1:0.6649\n",
      "   exact mode, Precision=0.6581, Recall=0.7002, F1:0.6727\n",
      " partial mode, Precision=0.7935, Recall=0.8398, F1:0.8087\n",
      "    type mode, Precision=0.9045, Recall=0.9529, F1:0.9199\n",
      "time used: 116 s\n",
      "epoch 1 =============\n",
      "iter 500: loss 5.707264\n",
      "iter 1000: loss 1.945801\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7317, Recall=0.7539, F1:0.7384\n",
      "   exact mode, Precision=0.7405, Recall=0.7616, F1:0.7463\n",
      " partial mode, Precision=0.8492, Recall=0.8714, F1:0.8547\n",
      "    type mode, Precision=0.9401, Recall=0.9646, F1:0.9466\n",
      "time used: 120 s\n",
      "epoch 2 =============\n",
      "iter 500: loss 3.551781\n",
      "iter 1000: loss 1.687943\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7694, Recall=0.7832, F1:0.7722\n",
      "   exact mode, Precision=0.7788, Recall=0.7910, F1:0.7805\n",
      " partial mode, Precision=0.8713, Recall=0.8833, F1:0.8721\n",
      "    type mode, Precision=0.9521, Recall=0.9651, F1:0.9530\n",
      "time used: 118 s\n",
      "epoch 3 =============\n",
      "iter 500: loss 2.648972\n",
      "iter 1000: loss 1.472755\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7769, Recall=0.7860, F1:0.7771\n",
      "   exact mode, Precision=0.7852, Recall=0.7921, F1:0.7840\n",
      " partial mode, Precision=0.8751, Recall=0.8816, F1:0.8731\n",
      "    type mode, Precision=0.9546, Recall=0.9623, F1:0.9528\n",
      "time used: 121 s\n",
      "epoch 4 =============\n",
      "iter 500: loss 2.165604\n",
      "iter 1000: loss 1.280380\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7802, Recall=0.7860, F1:0.7785\n",
      "   exact mode, Precision=0.7885, Recall=0.7921, F1:0.7853\n",
      " partial mode, Precision=0.8750, Recall=0.8780, F1:0.8709\n",
      "    type mode, Precision=0.9510, Recall=0.9551, F1:0.9471\n",
      "time used: 123 s\n",
      "epoch 5 =============\n",
      "iter 500: loss 1.824036\n",
      "iter 1000: loss 1.106407\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7821, Recall=0.7827, F1:0.7773\n",
      "   exact mode, Precision=0.7904, Recall=0.7888, F1:0.7842\n",
      " partial mode, Precision=0.8749, Recall=0.8733, F1:0.8677\n",
      "    type mode, Precision=0.9488, Recall=0.9491, F1:0.9420\n",
      "time used: 120 s\n",
      "epoch 6 =============\n",
      "iter 500: loss 1.612976\n",
      "iter 1000: loss 0.947433\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7821, Recall=0.7821, F1:0.7770\n",
      "   exact mode, Precision=0.7904, Recall=0.7882, F1:0.7838\n",
      " partial mode, Precision=0.8750, Recall=0.8725, F1:0.8674\n",
      "    type mode, Precision=0.9491, Recall=0.9480, F1:0.9416\n",
      "time used: 121 s\n",
      "epoch 7 =============\n",
      "iter 500: loss 1.392090\n",
      "iter 1000: loss 0.821571\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7877, Recall=0.7865, F1:0.7821\n",
      "   exact mode, Precision=0.7960, Recall=0.7926, F1:0.7889\n",
      " partial mode, Precision=0.8786, Recall=0.8747, F1:0.8705\n",
      "    type mode, Precision=0.9507, Recall=0.9480, F1:0.9427\n",
      "time used: 121 s\n",
      "epoch 8 =============\n",
      "iter 500: loss 1.241531\n",
      "iter 1000: loss 0.729149\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7904, Recall=0.7854, F1:0.7824\n",
      "   exact mode, Precision=0.7987, Recall=0.7915, F1:0.7893\n",
      " partial mode, Precision=0.8808, Recall=0.8722, F1:0.8697\n",
      "    type mode, Precision=0.9524, Recall=0.9441, F1:0.9408\n",
      "time used: 119 s\n",
      "epoch 9 =============\n",
      "iter 500: loss 1.148750\n",
      "iter 1000: loss 0.662010\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7943, Recall=0.7893, F1:0.7859\n",
      "   exact mode, Precision=0.8026, Recall=0.7954, F1:0.7928\n",
      " partial mode, Precision=0.8822, Recall=0.8739, F1:0.8709\n",
      "    type mode, Precision=0.9513, Recall=0.9435, F1:0.9397\n",
      "time used: 117 s\n",
      "epoch 10 =============\n",
      "iter 500: loss 1.091698\n",
      "iter 1000: loss 0.614052\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7921, Recall=0.7877, F1:0.7845\n",
      "   exact mode, Precision=0.8004, Recall=0.7937, F1:0.7914\n",
      " partial mode, Precision=0.8816, Recall=0.8742, F1:0.8713\n",
      "    type mode, Precision=0.9524, Recall=0.9457, F1:0.9418\n",
      "time used: 124 s\n",
      "epoch 11 =============\n",
      "iter 500: loss 1.055214\n",
      "iter 1000: loss 0.580528\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7865, Recall=0.7816, F1:0.7784\n",
      "   exact mode, Precision=0.7949, Recall=0.7877, F1:0.7853\n",
      " partial mode, Precision=0.8783, Recall=0.8700, F1:0.8672\n",
      "    type mode, Precision=0.9513, Recall=0.9435, F1:0.9399\n",
      "time used: 121 s\n",
      "epoch 12 =============\n",
      "iter 500: loss 1.029778\n",
      "iter 1000: loss 0.557732\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7854, Recall=0.7816, F1:0.7777\n",
      "   exact mode, Precision=0.7937, Recall=0.7877, F1:0.7846\n",
      " partial mode, Precision=0.8772, Recall=0.8700, F1:0.8665\n",
      "    type mode, Precision=0.9502, Recall=0.9435, F1:0.9392\n",
      "time used: 121 s\n",
      "epoch 13 =============\n",
      "iter 500: loss 1.009621\n",
      "iter 1000: loss 0.542755\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7899, Recall=0.7849, F1:0.7813\n",
      "   exact mode, Precision=0.7982, Recall=0.7910, F1:0.7881\n",
      " partial mode, Precision=0.8805, Recall=0.8725, F1:0.8691\n",
      "    type mode, Precision=0.9524, Recall=0.9452, F1:0.9407\n",
      "time used: 122 s\n",
      "epoch 14 =============\n",
      "iter 500: loss 0.991219\n",
      "iter 1000: loss 0.533478\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7865, Recall=0.7816, F1:0.7780\n",
      "   exact mode, Precision=0.7949, Recall=0.7877, F1:0.7848\n",
      " partial mode, Precision=0.8805, Recall=0.8717, F1:0.8685\n",
      "    type mode, Precision=0.9557, Recall=0.9468, F1:0.9430\n",
      "time used: 122 s\n"
     ]
    }
   ],
   "source": [
    "bert_crf = Bert_CRF(tag_to_ix, EMBEDDING_DIM)\n",
    "optimizer3 = optim.SGD(bert_crf.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "F1_bert_crf,bert_crf = train_bert(bert_crf,optimizer3,EPOCHS)\n",
    "torch.save(bert_crf.state_dict(),'./Model/bert_crf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa83cf32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T18:05:50.996580Z",
     "start_time": "2023-05-11T17:28:41.768947Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 =============\n",
      "iter 500: loss 9.204525\n",
      "iter 1000: loss 1.952400\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6440, Recall=0.6730, F1:0.6526\n",
      "   exact mode, Precision=0.6490, Recall=0.6791, F1:0.6581\n",
      " partial mode, Precision=0.7888, Recall=0.8220, F1:0.7972\n",
      "    type mode, Precision=0.9096, Recall=0.9427, F1:0.9165\n",
      "time used: 124 s\n",
      "epoch 1 =============\n",
      "iter 500: loss 4.199558\n",
      "iter 1000: loss 1.651016\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7325, Recall=0.7384, F1:0.7304\n",
      "   exact mode, Precision=0.7403, Recall=0.7445, F1:0.7371\n",
      " partial mode, Precision=0.8498, Recall=0.8527, F1:0.8448\n",
      "    type mode, Precision=0.9405, Recall=0.9444, F1:0.9354\n",
      "time used: 124 s\n",
      "epoch 2 =============\n",
      "iter 500: loss 3.457542\n",
      "iter 1000: loss 1.357452\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7544, Recall=0.7605, F1:0.7529\n",
      "   exact mode, Precision=0.7622, Recall=0.7666, F1:0.7595\n",
      " partial mode, Precision=0.8605, Recall=0.8648, F1:0.8567\n",
      "    type mode, Precision=0.9471, Recall=0.9513, F1:0.9429\n",
      "time used: 124 s\n",
      "epoch 3 =============\n",
      "iter 500: loss 2.388721\n",
      "iter 1000: loss 1.103340\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7688, Recall=0.7746, F1:0.7670\n",
      "   exact mode, Precision=0.7766, Recall=0.7807, F1:0.7736\n",
      " partial mode, Precision=0.8692, Recall=0.8725, F1:0.8649\n",
      "    type mode, Precision=0.9504, Recall=0.9538, F1:0.9457\n",
      "time used: 124 s\n",
      "epoch 4 =============\n",
      "iter 500: loss 1.640110\n",
      "iter 1000: loss 1.888779\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7307, Recall=0.7380, F1:0.7270\n",
      "   exact mode, Precision=0.7390, Recall=0.7430, F1:0.7331\n",
      " partial mode, Precision=0.8424, Recall=0.8492, F1:0.8368\n",
      "    type mode, Precision=0.9285, Recall=0.9405, F1:0.9251\n",
      "time used: 127 s\n",
      "epoch 5 =============\n",
      "iter 500: loss 5.586411\n",
      "iter 1000: loss 7.038628\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7608, Recall=0.7636, F1:0.7561\n",
      "   exact mode, Precision=0.7708, Recall=0.7713, F1:0.7646\n",
      " partial mode, Precision=0.8635, Recall=0.8631, F1:0.8556\n",
      "    type mode, Precision=0.9394, Recall=0.9383, F1:0.9305\n",
      "time used: 128 s\n",
      "epoch 6 =============\n",
      "iter 500: loss 1.004631\n",
      "iter 1000: loss 0.407257\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7885, Recall=0.7780, F1:0.7763\n",
      "   exact mode, Precision=0.7996, Recall=0.7857, F1:0.7850\n",
      " partial mode, Precision=0.8811, Recall=0.8664, F1:0.8656\n",
      "    type mode, Precision=0.9488, Recall=0.9366, F1:0.9347\n",
      "time used: 124 s\n",
      "epoch 7 =============\n",
      "iter 500: loss 0.663185\n",
      "iter 1000: loss 0.515816\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7907, Recall=0.7777, F1:0.7767\n",
      "   exact mode, Precision=0.8018, Recall=0.7854, F1:0.7854\n",
      " partial mode, Precision=0.8837, Recall=0.8646, F1:0.8652\n",
      "    type mode, Precision=0.9524, Recall=0.9333, F1:0.9338\n",
      "time used: 123 s\n",
      "epoch 8 =============\n",
      "iter 500: loss 0.621902\n",
      "iter 1000: loss 0.520439\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7913, Recall=0.7824, F1:0.7800\n",
      "   exact mode, Precision=0.8023, Recall=0.7901, F1:0.7888\n",
      " partial mode, Precision=0.8807, Recall=0.8681, F1:0.8661\n",
      "    type mode, Precision=0.9427, Recall=0.9311, F1:0.9286\n",
      "time used: 128 s\n",
      "epoch 9 =============\n",
      "iter 500: loss 0.548500\n",
      "iter 1000: loss 0.496216\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7744, Recall=0.7674, F1:0.7638\n",
      "   exact mode, Precision=0.7888, Recall=0.7769, F1:0.7748\n",
      " partial mode, Precision=0.8746, Recall=0.8620, F1:0.8592\n",
      "    type mode, Precision=0.9408, Recall=0.9305, F1:0.9267\n",
      "time used: 124 s\n",
      "epoch 10 =============\n",
      "iter 500: loss 0.509216\n",
      "iter 1000: loss 0.487389\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7937, Recall=0.7951, F1:0.7875\n",
      "   exact mode, Precision=0.8081, Recall=0.8045, F1:0.7985\n",
      " partial mode, Precision=0.8821, Recall=0.8789, F1:0.8716\n",
      "    type mode, Precision=0.9374, Recall=0.9383, F1:0.9290\n",
      "time used: 123 s\n",
      "epoch 11 =============\n",
      "iter 500: loss 0.647118\n",
      "iter 1000: loss 0.667633\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7835, Recall=0.7818, F1:0.7752\n",
      "   exact mode, Precision=0.7979, Recall=0.7913, F1:0.7861\n",
      " partial mode, Precision=0.8771, Recall=0.8706, F1:0.8644\n",
      "    type mode, Precision=0.9366, Recall=0.9333, F1:0.9257\n",
      "time used: 124 s\n",
      "epoch 12 =============\n",
      "iter 500: loss 0.661507\n",
      "iter 1000: loss 0.529663\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7818, Recall=0.7780, F1:0.7727\n",
      "   exact mode, Precision=0.7962, Recall=0.7874, F1:0.7837\n",
      " partial mode, Precision=0.8776, Recall=0.8684, F1:0.8639\n",
      "    type mode, Precision=0.9399, Recall=0.9338, F1:0.9279\n",
      "time used: 125 s\n",
      "epoch 13 =============\n",
      "iter 500: loss 0.510719\n",
      "iter 1000: loss 0.655151\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7821, Recall=0.7796, F1:0.7737\n",
      "   exact mode, Precision=0.7965, Recall=0.7890, F1:0.7847\n",
      " partial mode, Precision=0.8767, Recall=0.8667, F1:0.8624\n",
      "    type mode, Precision=0.9383, Recall=0.9288, F1:0.9242\n",
      "time used: 125 s\n",
      "epoch 14 =============\n",
      "iter 500: loss 0.484932\n",
      "iter 1000: loss 0.381409\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7838, Recall=0.7885, F1:0.7799\n",
      "   exact mode, Precision=0.7971, Recall=0.7962, F1:0.7896\n",
      " partial mode, Precision=0.8767, Recall=0.8756, F1:0.8679\n",
      "    type mode, Precision=0.9399, Recall=0.9427, F1:0.9330\n",
      "time used: 124 s\n"
     ]
    }
   ],
   "source": [
    "bert_att_crf = Bert_ATT_CRF(tag_to_ix, EMBEDDING_DIM)\n",
    "optimizer4 = optim.SGD(bert_att_crf.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "F1_bilstm_att_crf,bert_att_crf = train_bert(bert_att_crf,optimizer4,EPOCHS)\n",
    "torch.save(bert_att_crf.state_dict(),'./Model/bert_att_crf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b2d5f2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T10:33:16.855157Z",
     "start_time": "2023-05-26T09:55:26.674119Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 =============\n",
      "iter 500: loss 13.297150\n",
      "iter 1000: loss 3.740913\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6903, Recall=0.7283, F1:0.7043\n",
      "   exact mode, Precision=0.6997, Recall=0.7377, F1:0.7137\n",
      " partial mode, Precision=0.8175, Recall=0.8630, F1:0.8339\n",
      "    type mode, Precision=0.9132, Recall=0.9612, F1:0.9302\n",
      "time used: 131 s\n",
      "epoch 1 =============\n",
      "iter 500: loss 4.492828\n",
      "iter 1000: loss 2.787628\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7791, Recall=0.7937, F1:0.7818\n",
      "   exact mode, Precision=0.7879, Recall=0.8026, F1:0.7906\n",
      " partial mode, Precision=0.8739, Recall=0.8905, F1:0.8772\n",
      "    type mode, Precision=0.9455, Recall=0.9640, F1:0.9495\n",
      "time used: 124 s\n",
      "epoch 2 =============\n",
      "iter 500: loss 1.515541\n",
      "iter 1000: loss 1.545715\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7943, Recall=0.8070, F1:0.7969\n",
      "   exact mode, Precision=0.8020, Recall=0.8148, F1:0.8047\n",
      " partial mode, Precision=0.8826, Recall=0.8963, F1:0.8853\n",
      "    type mode, Precision=0.9535, Recall=0.9679, F1:0.9561\n",
      "time used: 126 s\n",
      "epoch 3 =============\n",
      "iter 500: loss 0.822693\n",
      "iter 1000: loss 1.964508\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7904, Recall=0.7960, F1:0.7890\n",
      "   exact mode, Precision=0.8009, Recall=0.8048, F1:0.7984\n",
      " partial mode, Precision=0.8826, Recall=0.8872, F1:0.8801\n",
      "    type mode, Precision=0.9538, Recall=0.9607, F1:0.9524\n",
      "time used: 128 s\n",
      "epoch 4 =============\n",
      "iter 500: loss 0.873085\n",
      "iter 1000: loss 1.415932\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.8098, Recall=0.8034, F1:0.8018\n",
      "   exact mode, Precision=0.8176, Recall=0.8095, F1:0.8085\n",
      " partial mode, Precision=0.8942, Recall=0.8844, F1:0.8837\n",
      "    type mode, Precision=0.9632, Recall=0.9532, F1:0.9523\n",
      "time used: 125 s\n",
      "epoch 5 =============\n",
      "iter 500: loss 0.483490\n",
      "iter 1000: loss 1.174980\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7984, Recall=0.7877, F1:0.7889\n",
      "   exact mode, Precision=0.8068, Recall=0.7926, F1:0.7949\n",
      " partial mode, Precision=0.8901, Recall=0.8744, F1:0.8770\n",
      "    type mode, Precision=0.9651, Recall=0.9513, F1:0.9530\n",
      "time used: 125 s\n",
      "epoch 6 =============\n",
      "iter 500: loss 0.385811\n",
      "iter 1000: loss 0.820877\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.8314, Recall=0.8070, F1:0.8130\n",
      "   exact mode, Precision=0.8364, Recall=0.8104, F1:0.8169\n",
      " partial mode, Precision=0.9066, Recall=0.8792, F1:0.8860\n",
      "    type mode, Precision=0.9718, Recall=0.9446, F1:0.9513\n",
      "time used: 130 s\n",
      "epoch 7 =============\n",
      "iter 500: loss 0.377762\n",
      "iter 1000: loss 0.392761\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.8397, Recall=0.8059, F1:0.8154\n",
      "   exact mode, Precision=0.8447, Recall=0.8092, F1:0.8192\n",
      " partial mode, Precision=0.9122, Recall=0.8758, F1:0.8860\n",
      "    type mode, Precision=0.9748, Recall=0.9391, F1:0.9489\n",
      "time used: 137 s\n",
      "epoch 8 =============\n",
      "iter 500: loss 0.517662\n",
      "iter 1000: loss 0.381775\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.8281, Recall=0.8009, F1:0.8083\n",
      "   exact mode, Precision=0.8331, Recall=0.8043, F1:0.8121\n",
      " partial mode, Precision=0.9010, Recall=0.8706, F1:0.8788\n",
      "    type mode, Precision=0.9640, Recall=0.9336, F1:0.9416\n",
      "time used: 132 s\n",
      "epoch 9 =============\n",
      "iter 500: loss 0.273453\n",
      "iter 1000: loss 0.451469\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.8275, Recall=0.7738, F1:0.7911\n",
      "   exact mode, Precision=0.8303, Recall=0.7766, F1:0.7939\n",
      " partial mode, Precision=0.8996, Recall=0.8445, F1:0.8622\n",
      "    type mode, Precision=0.9662, Recall=0.9097, F1:0.9278\n",
      "time used: 133 s\n",
      "epoch 10 =============\n",
      "iter 500: loss 0.439407\n",
      "iter 1000: loss 0.237267\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.8131, Recall=0.7860, F1:0.7933\n",
      "   exact mode, Precision=0.8181, Recall=0.7893, F1:0.7972\n",
      " partial mode, Precision=0.8933, Recall=0.8623, F1:0.8707\n",
      "    type mode, Precision=0.9635, Recall=0.9319, F1:0.9402\n",
      "time used: 132 s\n",
      "epoch 11 =============\n",
      "iter 500: loss 0.168182\n",
      "iter 1000: loss 0.093376\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.8281, Recall=0.7982, F1:0.8066\n",
      "   exact mode, Precision=0.8331, Recall=0.8015, F1:0.8105\n",
      " partial mode, Precision=0.9061, Recall=0.8728, F1:0.8822\n",
      "    type mode, Precision=0.9743, Recall=0.9408, F1:0.9500\n",
      "time used: 129 s\n",
      "epoch 12 =============\n",
      "iter 500: loss 0.215775\n",
      "iter 1000: loss 0.358116\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.8270, Recall=0.7921, F1:0.8025\n",
      "   exact mode, Precision=0.8319, Recall=0.7954, F1:0.8064\n",
      " partial mode, Precision=0.9041, Recall=0.8645, F1:0.8763\n",
      "    type mode, Precision=0.9712, Recall=0.9302, F1:0.9423\n",
      "time used: 123 s\n",
      "epoch 13 =============\n",
      "iter 500: loss 0.203918\n",
      "iter 1000: loss 0.134628\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.8292, Recall=0.7987, F1:0.8072\n",
      "   exact mode, Precision=0.8358, Recall=0.8032, F1:0.8124\n",
      " partial mode, Precision=0.9063, Recall=0.8736, F1:0.8826\n",
      "    type mode, Precision=0.9690, Recall=0.9380, F1:0.9462\n",
      "time used: 128 s\n",
      "epoch 14 =============\n",
      "iter 500: loss 0.138580\n",
      "iter 1000: loss 0.285706\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.8140, Recall=0.7788, F1:0.7895\n",
      "   exact mode, Precision=0.8206, Recall=0.7832, F1:0.7947\n",
      " partial mode, Precision=0.9005, Recall=0.8584, F1:0.8711\n",
      "    type mode, Precision=0.9737, Recall=0.9291, F1:0.9423\n",
      "time used: 123 s\n"
     ]
    }
   ],
   "source": [
    "bert_bilstm_crf = Bert_BiLSTM_CRF(tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer5 = optim.SGD(bert_bilstm_crf.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "F1_bert_bilstm_crf,bert_bilstm_crf = train_bert(bert_bilstm_crf,optimizer5,EPOCHS)\n",
    "torch.save(bert_bilstm_crf.state_dict(),'./Model/bert_bilstm_crf_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b6196d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T12:04:13.788900Z",
     "start_time": "2023-05-26T12:04:13.749902Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('transitions',\n",
       "              tensor([[ 3.5112e+00, -8.1621e-01,  1.5422e-02, -8.9862e-01,  1.2429e+00,\n",
       "                       -1.4511e+00, -1.0000e+04],\n",
       "                      [ 1.3920e+00, -7.5281e-01,  5.5016e-02, -8.8791e-02, -6.7963e-01,\n",
       "                        7.7156e-01, -1.0000e+04],\n",
       "                      [-2.8223e+00,  1.2693e+00,  1.5500e+00, -3.2786e-02,  9.1519e-01,\n",
       "                        1.2727e-01, -1.0000e+04],\n",
       "                      [ 1.6845e+00,  1.3555e+00, -3.9388e-01, -1.7229e+00, -2.3801e+00,\n",
       "                        5.4012e-01, -1.0000e+04],\n",
       "                      [-1.3457e+00,  3.5078e-01, -1.4227e+00,  6.2656e-01,  2.4739e+00,\n",
       "                       -4.4986e-01, -1.0000e+04],\n",
       "                      [-1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "                       -1.0000e+04, -1.0000e+04],\n",
       "                      [ 2.5859e+00, -1.0751e+00,  1.3245e+00, -7.9399e-01,  1.0516e+00,\n",
       "                        5.6089e-01, -1.0000e+04]])),\n",
       "             ('word_embeds.weight',\n",
       "              tensor([[ 0.0242,  0.0080,  0.0471,  ..., -0.0346,  0.0067, -0.0199],\n",
       "                      [ 0.0141, -0.0089, -0.0096,  ..., -0.0495,  0.0121, -0.0072],\n",
       "                      [ 0.0169,  0.0318, -0.0279,  ...,  0.0484,  0.0277, -0.0181],\n",
       "                      ...,\n",
       "                      [-0.0376,  0.0276,  0.0143,  ...,  0.0243, -0.0064,  0.0127],\n",
       "                      [ 0.0497,  0.0348,  0.0416,  ..., -0.0071, -0.0052, -0.0412],\n",
       "                      [-0.0427, -0.0234,  0.0251,  ...,  0.0025,  0.0208, -0.0168]])),\n",
       "             ('word_embeds.bias',\n",
       "              tensor([-1.0979e-02,  1.8718e-03, -3.6291e-02,  3.0864e-02, -3.5680e-02,\n",
       "                       3.4481e-02, -9.7925e-05, -2.1643e-02, -3.7655e-02,  4.4029e-03,\n",
       "                       8.4519e-03,  2.4634e-02, -5.9903e-03,  2.7563e-04,  3.8995e-02,\n",
       "                      -1.5771e-02, -2.0267e-02,  2.8903e-02,  1.8492e-02,  2.7043e-02,\n",
       "                      -1.0291e-02, -1.1927e-02, -2.9099e-02,  2.3591e-02, -2.7369e-02,\n",
       "                      -4.2647e-02, -1.0827e-02, -2.7499e-02,  6.2191e-03, -1.4244e-02,\n",
       "                       2.5589e-02,  3.1894e-02,  1.5340e-02,  6.6481e-03, -2.5910e-02,\n",
       "                      -2.1536e-02,  2.8230e-03,  7.2320e-03,  2.9621e-03,  2.8998e-02,\n",
       "                      -2.2570e-02,  7.4669e-04, -2.3146e-02, -3.4337e-02, -3.4513e-02,\n",
       "                      -2.2106e-02, -2.0692e-02,  1.0951e-02,  2.7491e-02, -2.8766e-03,\n",
       "                      -3.7469e-02,  3.3639e-02, -1.7892e-02,  4.1460e-03,  5.0057e-03,\n",
       "                       1.9336e-02,  2.2972e-02,  2.2411e-02, -4.6361e-03,  3.1668e-02,\n",
       "                       1.9547e-02, -1.7081e-02,  2.2483e-02, -1.6105e-02, -3.0452e-02,\n",
       "                       1.9669e-02,  5.8481e-03,  2.6852e-02, -1.7033e-02, -2.8335e-02,\n",
       "                       3.0267e-03, -1.7447e-02, -8.5666e-03, -3.6577e-02,  2.3406e-05,\n",
       "                       1.0532e-02, -3.4376e-02, -1.2523e-02,  1.1979e-02,  3.0852e-02,\n",
       "                       2.7289e-02,  1.2193e-02,  2.1310e-02,  2.1627e-02,  1.4915e-02,\n",
       "                      -2.2483e-02, -1.4305e-02,  9.9732e-03,  3.2404e-02,  4.2694e-03,\n",
       "                       3.0630e-02, -8.1501e-03,  3.2541e-02, -3.8107e-02,  1.7106e-02,\n",
       "                      -2.0271e-02, -2.8622e-02, -2.4093e-02,  5.8156e-03, -2.0435e-02,\n",
       "                      -1.7657e-02, -1.9494e-02, -1.6554e-02,  2.7867e-02,  2.3684e-02,\n",
       "                      -8.5357e-03, -2.4069e-03,  1.8658e-03,  5.9115e-03,  2.2494e-02,\n",
       "                      -1.6046e-02,  1.7460e-02, -2.7670e-02, -3.1763e-02, -4.2077e-03,\n",
       "                       3.0281e-03, -1.0010e-02, -2.5071e-03, -2.2613e-03,  2.4534e-02,\n",
       "                      -1.4622e-02,  2.8824e-02,  1.0342e-02, -2.3055e-02,  3.1789e-02,\n",
       "                       1.6065e-03,  8.7896e-03,  8.4179e-03])),\n",
       "             ('lstm.weight_ih_l0',\n",
       "              tensor([[ 0.0732,  0.0347,  0.0639,  ..., -0.0162,  0.0935, -0.0701],\n",
       "                      [ 0.0671, -0.0257, -0.0427,  ..., -0.0803,  0.0167,  0.0962],\n",
       "                      [-0.0032,  0.0558,  0.1045,  ..., -0.0588, -0.0227, -0.0792],\n",
       "                      ...,\n",
       "                      [ 0.0298, -0.1211, -0.0728,  ..., -0.0377, -0.0261,  0.0101],\n",
       "                      [ 0.1091, -0.0628, -0.0747,  ...,  0.0613,  0.0863, -0.0565],\n",
       "                      [-0.1084,  0.0894, -0.0565,  ..., -0.0552,  0.0766, -0.0223]])),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[-0.0250,  0.1171,  0.1214,  ...,  0.1074, -0.0301,  0.1256],\n",
       "                      [-0.1259,  0.0065,  0.0280,  ..., -0.1157,  0.0665, -0.0907],\n",
       "                      [-0.1016,  0.1207, -0.0998,  ..., -0.1088,  0.0573,  0.0833],\n",
       "                      ...,\n",
       "                      [-0.1025, -0.0510, -0.0500,  ..., -0.0301, -0.1193,  0.0118],\n",
       "                      [ 0.0447, -0.0543, -0.0306,  ...,  0.1230,  0.1014,  0.0809],\n",
       "                      [ 0.0407,  0.0738, -0.1179,  ..., -0.0032, -0.0402, -0.0278]])),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([-0.0686, -0.0283, -0.0886,  0.0852, -0.0568, -0.0504, -0.0277, -0.0619,\n",
       "                       0.0612,  0.0624,  0.0705, -0.0990,  0.0507, -0.0037,  0.0853,  0.0940,\n",
       "                       0.0109, -0.1057,  0.1313,  0.1157,  0.0031,  0.1185,  0.0628, -0.0982,\n",
       "                      -0.0408,  0.1103,  0.1008,  0.1175,  0.0957,  0.1477, -0.0694,  0.0064,\n",
       "                       0.0739,  0.0202, -0.0462, -0.0798,  0.0098, -0.0745,  0.0819, -0.0301,\n",
       "                       0.0513,  0.1038, -0.1055, -0.0792, -0.0616,  0.0688,  0.0669,  0.0976,\n",
       "                      -0.0439, -0.0656, -0.0149, -0.0028,  0.0218, -0.0268,  0.0020,  0.0065,\n",
       "                       0.1204, -0.0231,  0.1286, -0.0367,  0.0958,  0.0624, -0.1127, -0.1011,\n",
       "                      -0.0133,  0.0762, -0.0038,  0.1190,  0.0411, -0.0672,  0.0169, -0.0857,\n",
       "                      -0.0504,  0.0591,  0.0542, -0.0845,  0.0238,  0.0127, -0.1261,  0.0441,\n",
       "                      -0.0138, -0.1020,  0.1066, -0.0030, -0.0389, -0.0947, -0.0746, -0.0525,\n",
       "                      -0.0193,  0.0667,  0.1077,  0.0885, -0.0123,  0.0805, -0.0657,  0.0198,\n",
       "                      -0.0958,  0.0570,  0.0993, -0.1207, -0.1194, -0.0662, -0.1055, -0.0567,\n",
       "                      -0.1200, -0.0336,  0.0494, -0.0339, -0.0516,  0.1057,  0.0259, -0.0161,\n",
       "                      -0.1064,  0.0290,  0.1105,  0.0988, -0.0405,  0.0653, -0.0929, -0.1027,\n",
       "                      -0.0402, -0.0873,  0.0963,  0.0613, -0.0581, -0.0021,  0.0497,  0.0580,\n",
       "                       0.1083, -0.0104, -0.0585, -0.0329,  0.0285, -0.0603,  0.1224, -0.0347,\n",
       "                      -0.0111,  0.0854,  0.0349, -0.0745, -0.0312, -0.1238, -0.0501,  0.0757,\n",
       "                       0.1175,  0.0558, -0.1155,  0.0293,  0.1192, -0.0665,  0.0036,  0.0771,\n",
       "                      -0.1240, -0.0427, -0.0118, -0.0660,  0.0294,  0.0663, -0.0508,  0.0259,\n",
       "                      -0.0461, -0.0012,  0.0675,  0.0791,  0.0406,  0.0272,  0.0591, -0.0196,\n",
       "                      -0.1122, -0.0742,  0.0610, -0.0263, -0.0646,  0.0759,  0.1229, -0.0816,\n",
       "                      -0.0082, -0.0794,  0.0077,  0.1150, -0.0342,  0.0631,  0.1088,  0.1153,\n",
       "                       0.0958,  0.0257,  0.0650,  0.0387, -0.0447,  0.1043, -0.0249, -0.0658,\n",
       "                       0.0152, -0.1129, -0.0107, -0.0994,  0.0686,  0.0257,  0.0132,  0.0760,\n",
       "                       0.0626,  0.0696, -0.0584, -0.0518,  0.0639, -0.0041, -0.1086, -0.0637,\n",
       "                      -0.0774, -0.1111,  0.0768,  0.0639,  0.0230, -0.0849, -0.0435,  0.0882,\n",
       "                      -0.1086,  0.0995, -0.0592, -0.0908,  0.0434,  0.1310,  0.0670,  0.0955,\n",
       "                       0.1083, -0.0168,  0.0527,  0.1170,  0.0402, -0.0511,  0.0061,  0.0459,\n",
       "                      -0.0792,  0.0085,  0.0981,  0.0342,  0.0445,  0.0424, -0.0483,  0.0967,\n",
       "                       0.0732, -0.0065,  0.0936,  0.0056,  0.0680,  0.1410, -0.0922, -0.0365,\n",
       "                       0.0458,  0.0892,  0.0844,  0.0041, -0.0187,  0.0482,  0.0601,  0.0315])),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([ 0.1195, -0.0437,  0.0709, -0.0372,  0.0659,  0.0475,  0.1175, -0.0316,\n",
       "                       0.0775, -0.0534,  0.0264, -0.1202,  0.1126, -0.0289,  0.0600,  0.0218,\n",
       "                       0.0108, -0.0936, -0.1042,  0.1052, -0.1223, -0.0613, -0.0567, -0.1064,\n",
       "                      -0.0456,  0.1206,  0.0159,  0.0143, -0.0846, -0.0632, -0.0241,  0.0949,\n",
       "                      -0.0691,  0.0482,  0.0022,  0.0473, -0.0930,  0.0950,  0.0332,  0.0760,\n",
       "                       0.1206, -0.0312, -0.0449,  0.0486,  0.1154,  0.0166, -0.0082, -0.0836,\n",
       "                      -0.1057,  0.0166,  0.1009,  0.0134,  0.1178,  0.0273,  0.0308, -0.0253,\n",
       "                      -0.0342,  0.1042, -0.0107,  0.0745,  0.0241, -0.0506,  0.0112,  0.0499,\n",
       "                       0.1112,  0.0332, -0.1142, -0.0717,  0.0933, -0.0935, -0.0171,  0.1140,\n",
       "                       0.0953,  0.0016,  0.0323, -0.0604, -0.0256, -0.0055, -0.1185,  0.0247,\n",
       "                       0.0676,  0.1118, -0.0021,  0.0557, -0.1229,  0.0778,  0.0197,  0.0306,\n",
       "                      -0.1175, -0.1164,  0.1198,  0.1049,  0.1253, -0.0613, -0.0150, -0.1199,\n",
       "                       0.0627,  0.0331,  0.0333, -0.0765, -0.0486, -0.0882,  0.0126,  0.0261,\n",
       "                      -0.0562, -0.1346,  0.0290, -0.0928, -0.0740, -0.1248, -0.0207,  0.0434,\n",
       "                       0.0308,  0.0251,  0.0081,  0.0788,  0.0189, -0.0179, -0.0075,  0.0021,\n",
       "                       0.0020, -0.0369, -0.0514,  0.0801, -0.0867, -0.0707, -0.0840,  0.0731,\n",
       "                      -0.0124, -0.0198,  0.0104, -0.1191, -0.0870, -0.0299, -0.0849, -0.0489,\n",
       "                      -0.0194,  0.0740,  0.0329, -0.0862,  0.0481, -0.0307,  0.0582,  0.1185,\n",
       "                      -0.0494,  0.0275, -0.0821, -0.0925,  0.1140, -0.0927, -0.0290, -0.0970,\n",
       "                       0.0738, -0.0729,  0.0081, -0.0154,  0.0193, -0.0853, -0.0988,  0.0483,\n",
       "                       0.0092,  0.0395,  0.0458,  0.0673, -0.0887, -0.0457, -0.0098,  0.1128,\n",
       "                       0.0388, -0.0993,  0.0475, -0.0877,  0.0520, -0.0342,  0.0182,  0.0124,\n",
       "                      -0.0651,  0.1226, -0.0531, -0.1058,  0.0553, -0.1311,  0.0368, -0.1086,\n",
       "                       0.0591, -0.0210,  0.0013,  0.1052,  0.0305,  0.1277, -0.0315, -0.0678,\n",
       "                       0.0921, -0.0624, -0.0798, -0.0851,  0.0133, -0.0028,  0.0482,  0.0511,\n",
       "                       0.0634,  0.0773,  0.0362, -0.1067,  0.0560, -0.1078,  0.1328, -0.0682,\n",
       "                       0.0327, -0.0061, -0.0815,  0.0181, -0.1209,  0.0149,  0.0251, -0.1068,\n",
       "                       0.1298, -0.0917,  0.0453,  0.1236, -0.0122, -0.0479,  0.0178, -0.0587,\n",
       "                       0.1127,  0.0561, -0.0881,  0.0770,  0.0295, -0.0425,  0.1247, -0.1013,\n",
       "                      -0.0273,  0.0440,  0.0832, -0.0698,  0.0052,  0.0984,  0.0536, -0.0135,\n",
       "                      -0.0235,  0.0051, -0.1055, -0.0194,  0.0507, -0.0820,  0.0046, -0.1097,\n",
       "                      -0.1057, -0.0060,  0.1128, -0.0471,  0.1310, -0.1149,  0.1025,  0.0008])),\n",
       "             ('lstm.weight_ih_l0_reverse',\n",
       "              tensor([[ 1.1420e-03, -9.8296e-02,  2.8749e-02,  ..., -1.1458e-01,\n",
       "                        8.6472e-03, -1.0855e-01],\n",
       "                      [-7.9629e-02,  9.8496e-02, -8.8463e-02,  ..., -5.7469e-02,\n",
       "                        6.5683e-02,  1.0401e-01],\n",
       "                      [-1.2667e-02, -8.8714e-02,  1.1125e-01,  ...,  1.0661e-01,\n",
       "                       -3.8704e-02, -1.2388e-01],\n",
       "                      ...,\n",
       "                      [ 1.0089e-01, -1.5811e-02,  3.8873e-02,  ...,  3.9257e-04,\n",
       "                       -3.7871e-03,  1.0012e-01],\n",
       "                      [-1.1613e-01, -8.0352e-03, -1.1236e-04,  ..., -1.0574e-01,\n",
       "                        6.5226e-03,  8.0213e-03],\n",
       "                      [ 5.8415e-02, -2.3113e-03,  9.3120e-02,  ..., -9.1143e-02,\n",
       "                       -7.1586e-02,  1.0926e-01]])),\n",
       "             ('lstm.weight_hh_l0_reverse',\n",
       "              tensor([[ 0.0894, -0.0089, -0.0243,  ...,  0.0040, -0.0245, -0.0281],\n",
       "                      [ 0.0478,  0.0065, -0.0030,  ...,  0.0847, -0.0197,  0.1124],\n",
       "                      [ 0.0596,  0.0042, -0.1108,  ...,  0.0290,  0.0935, -0.0724],\n",
       "                      ...,\n",
       "                      [-0.0492, -0.0375, -0.0202,  ..., -0.0636, -0.1210, -0.0332],\n",
       "                      [-0.0242,  0.0621,  0.0555,  ...,  0.0943, -0.0896, -0.0579],\n",
       "                      [ 0.0693,  0.0299, -0.0510,  ..., -0.0257, -0.0635,  0.0849]])),\n",
       "             ('lstm.bias_ih_l0_reverse',\n",
       "              tensor([ 0.0615, -0.0089, -0.0538,  0.1198,  0.0409,  0.1024, -0.1081, -0.0307,\n",
       "                       0.1433, -0.0331, -0.0502, -0.0234,  0.0345,  0.0701, -0.0250,  0.0994,\n",
       "                       0.1406,  0.1161, -0.0989,  0.0362, -0.0121,  0.0207,  0.0076, -0.1192,\n",
       "                      -0.0239,  0.0127, -0.1086, -0.0961,  0.0807,  0.0302,  0.0317, -0.0672,\n",
       "                      -0.0277, -0.1040, -0.0491,  0.0160, -0.0384, -0.0041, -0.0672, -0.1172,\n",
       "                      -0.0791, -0.0482, -0.0350,  0.0022,  0.0638, -0.0030,  0.1298, -0.0926,\n",
       "                      -0.0577, -0.0156,  0.0545, -0.1099, -0.0269, -0.1018, -0.0715,  0.0005,\n",
       "                       0.0307, -0.0209,  0.1304, -0.0025, -0.1268, -0.0371, -0.0583,  0.1228,\n",
       "                      -0.0329, -0.0936, -0.0742,  0.1098, -0.0322, -0.0032, -0.0908,  0.0611,\n",
       "                      -0.0261,  0.0093,  0.0889,  0.0847, -0.1425,  0.0736,  0.1187,  0.1151,\n",
       "                       0.0833, -0.0616,  0.0042,  0.1159,  0.0889,  0.0761,  0.1185, -0.0091,\n",
       "                      -0.0585,  0.0894, -0.0774, -0.0790, -0.1317, -0.0790, -0.0175, -0.0937,\n",
       "                       0.1023,  0.0051, -0.0758,  0.0724, -0.1147,  0.0750,  0.0506,  0.0997,\n",
       "                       0.1181,  0.0528,  0.1075, -0.0959, -0.0897,  0.0730, -0.0492, -0.0357,\n",
       "                       0.0346,  0.0131, -0.0449,  0.0893, -0.0739,  0.0237, -0.1087, -0.0542,\n",
       "                      -0.0644,  0.1146,  0.1063, -0.1064,  0.0384,  0.0238, -0.0242, -0.0612,\n",
       "                       0.0866, -0.0720, -0.0626, -0.0090, -0.0131,  0.0845,  0.1020,  0.1086,\n",
       "                      -0.0655,  0.0873, -0.1244, -0.0576,  0.0110,  0.0802,  0.0428,  0.0749,\n",
       "                      -0.0922,  0.1150, -0.0371, -0.0098,  0.0866, -0.0075,  0.0023, -0.1070,\n",
       "                      -0.0643,  0.0361, -0.0438, -0.0074, -0.0656, -0.1041,  0.0764, -0.0663,\n",
       "                       0.1105,  0.0850, -0.0349, -0.0474, -0.0978, -0.0709,  0.1142, -0.0376,\n",
       "                      -0.0083,  0.0442, -0.1136,  0.0868, -0.1231, -0.0863, -0.0721,  0.0966,\n",
       "                       0.0199,  0.0687,  0.1093, -0.0959, -0.0746,  0.0604,  0.0127, -0.0624,\n",
       "                      -0.1146,  0.0200,  0.0899, -0.0843,  0.0194, -0.0277, -0.0092, -0.0130,\n",
       "                      -0.0210,  0.1096,  0.0385, -0.0966,  0.0211, -0.0965, -0.0565, -0.0591,\n",
       "                       0.0725,  0.0792,  0.0619,  0.1065, -0.0283,  0.1006,  0.0644,  0.0653,\n",
       "                      -0.0540, -0.0655,  0.0038,  0.0607,  0.0132, -0.0597,  0.0819, -0.0550,\n",
       "                      -0.0576,  0.0703,  0.0559,  0.0216, -0.0636, -0.0613, -0.0733,  0.0928,\n",
       "                       0.0015,  0.0563,  0.0002, -0.0363, -0.1153, -0.0980,  0.0943,  0.0141,\n",
       "                      -0.0326,  0.1026,  0.0984,  0.0112,  0.0903,  0.0166, -0.0236, -0.0310,\n",
       "                       0.0737, -0.1019, -0.0393,  0.0669,  0.0239, -0.1115, -0.0588,  0.1376,\n",
       "                       0.0679, -0.0924,  0.0680,  0.0072, -0.0661,  0.0452, -0.0707,  0.0339])),\n",
       "             ('lstm.bias_hh_l0_reverse',\n",
       "              tensor([ 0.1408, -0.0588, -0.1004, -0.0036,  0.1127,  0.0040,  0.0177, -0.1001,\n",
       "                       0.0724, -0.0049, -0.1134, -0.0424,  0.1061, -0.0120, -0.0988,  0.0879,\n",
       "                       0.1207,  0.0087,  0.0275, -0.0913, -0.0129, -0.1040, -0.1001, -0.0635,\n",
       "                       0.1310, -0.0697,  0.0720,  0.0388,  0.0333, -0.0014,  0.1402,  0.0206,\n",
       "                       0.0754,  0.0788, -0.0838,  0.0023, -0.1227,  0.1392,  0.0791, -0.0737,\n",
       "                      -0.0701, -0.0601,  0.1068,  0.0694, -0.0083,  0.1304, -0.0204, -0.0653,\n",
       "                      -0.0875,  0.1002,  0.0481, -0.0744,  0.0894,  0.0010, -0.0408, -0.0630,\n",
       "                      -0.1012, -0.1214,  0.1174,  0.1054, -0.0153,  0.0961, -0.0921, -0.0457,\n",
       "                       0.1225, -0.1068,  0.0222,  0.1028,  0.0097, -0.0553,  0.0549, -0.0369,\n",
       "                      -0.0884,  0.0650,  0.0940, -0.0732,  0.1006,  0.1237,  0.0312, -0.0533,\n",
       "                      -0.0537,  0.0456,  0.0317,  0.0630, -0.0358,  0.0669,  0.1124, -0.0431,\n",
       "                       0.0575, -0.0832, -0.0570, -0.0590,  0.0795, -0.0165,  0.0293,  0.0829,\n",
       "                       0.0434,  0.0126,  0.0313, -0.1012,  0.1158,  0.0989,  0.0032,  0.0801,\n",
       "                       0.0085,  0.0394,  0.0952, -0.0836, -0.0646,  0.0647,  0.0579, -0.1181,\n",
       "                       0.0128, -0.0637,  0.1056,  0.0856,  0.0074,  0.0495, -0.0689, -0.0272,\n",
       "                      -0.0268, -0.0949,  0.0965, -0.0465, -0.0109, -0.0760,  0.0738,  0.0948,\n",
       "                       0.0516,  0.0110, -0.0267,  0.1158, -0.0049,  0.0006,  0.0471, -0.0909,\n",
       "                      -0.0513, -0.1004, -0.0803,  0.0198, -0.0058,  0.1227,  0.1109,  0.0247,\n",
       "                      -0.0423,  0.0818,  0.0500,  0.0331, -0.0758,  0.0878,  0.1182,  0.0195,\n",
       "                      -0.0189,  0.0490, -0.0511, -0.0361, -0.0905, -0.0244,  0.0740,  0.1065,\n",
       "                       0.0492, -0.1083,  0.0426, -0.0304,  0.0857, -0.0273, -0.0659,  0.0581,\n",
       "                      -0.1097,  0.1006, -0.0094, -0.0897,  0.0217, -0.0345,  0.0972, -0.1235,\n",
       "                      -0.0890, -0.0199, -0.0719, -0.0396, -0.0309,  0.1096,  0.0932,  0.0504,\n",
       "                       0.0801,  0.0373, -0.0708,  0.0289, -0.0210,  0.0797, -0.0130, -0.0072,\n",
       "                      -0.0148,  0.0892,  0.0850,  0.0818,  0.0836,  0.1296,  0.1304,  0.0749,\n",
       "                       0.1287,  0.1205, -0.0542, -0.0103,  0.1077, -0.0005, -0.1186, -0.0343,\n",
       "                      -0.0399, -0.1090,  0.0635,  0.0423,  0.1184, -0.0336, -0.0371,  0.0154,\n",
       "                      -0.0628,  0.0960, -0.0556,  0.0535, -0.0524,  0.0996,  0.0653,  0.0981,\n",
       "                       0.0710, -0.0066,  0.0247, -0.0828,  0.0804,  0.1313, -0.0079,  0.0822,\n",
       "                       0.0688,  0.1268,  0.1208, -0.1043, -0.0375,  0.0107, -0.0539,  0.0860,\n",
       "                       0.0876, -0.0644, -0.0949,  0.0286,  0.0720, -0.0241, -0.0090,  0.1486,\n",
       "                      -0.0968, -0.0229, -0.0031,  0.1081, -0.0841,  0.1040, -0.1246, -0.0143])),\n",
       "             ('hidden2tag.weight',\n",
       "              tensor([[ 4.6552e-01, -6.5465e-02,  5.4528e-02, -8.7872e-03,  1.3357e-01,\n",
       "                       -3.9789e-01, -7.3537e-02,  2.7833e-01, -6.3473e-01, -2.9584e-02,\n",
       "                        8.2941e-02,  3.7242e-02,  7.1576e-02, -7.6136e-03, -5.6942e-02,\n",
       "                        1.3064e-01,  1.4318e-01, -6.7440e-02,  1.1411e-01,  1.5904e-01,\n",
       "                        1.8401e-02, -1.5605e-01, -7.3447e-02, -2.3479e-01,  1.1959e-01,\n",
       "                       -1.4881e-01, -1.3348e-01,  1.7493e-01,  5.8041e-02, -8.3194e-03,\n",
       "                       -2.9362e-01,  1.1419e-01, -4.7911e-02, -8.8777e-02,  3.3456e-01,\n",
       "                        3.2292e-01,  2.7850e-02, -6.2130e-02, -4.3040e-01,  2.5552e-02,\n",
       "                       -3.0772e-01, -5.9455e-01,  1.4627e-01, -3.0569e-01, -6.3257e-02,\n",
       "                        1.0644e-02, -1.3586e-01, -9.4736e-02, -1.1466e-01,  6.8077e-02,\n",
       "                       -2.4746e-02, -3.7041e-02,  5.2519e-02,  1.8381e-01, -5.3183e-02,\n",
       "                        3.1813e-01, -1.3894e-01, -9.2162e-02, -8.3249e-02,  1.1547e-01,\n",
       "                        3.2147e-02,  1.8792e-01, -2.6405e-02, -6.9764e-02,  1.3056e-01,\n",
       "                        9.1666e-02, -3.0752e-01, -1.0925e-02, -1.5336e-01,  3.5582e-01,\n",
       "                       -2.1916e-01, -1.0927e-01, -4.1297e-01, -2.1946e-01, -1.6374e-01,\n",
       "                        3.0538e-02, -5.0090e-01,  5.1104e-01,  1.4130e-01,  2.2386e-01,\n",
       "                        4.8626e-01, -1.3422e-01, -9.0144e-02,  2.3048e-01,  2.1753e-01,\n",
       "                       -5.6710e-02,  1.3317e-01, -2.3255e-01,  3.9937e-02, -4.9452e-02,\n",
       "                       -1.2360e-01, -6.2556e-02, -1.7371e-01,  1.1929e-01, -4.6206e-01,\n",
       "                        1.0820e-03, -3.1359e-01, -3.9721e-01,  4.1029e-03, -2.0151e-01,\n",
       "                        6.9518e-02, -1.2546e-02,  2.5066e-01, -1.1855e-01,  2.7407e-01,\n",
       "                        1.5516e-01,  1.5117e-01, -3.6186e-02,  5.1162e-02,  3.7546e-01,\n",
       "                        3.6294e-01,  9.9967e-03, -2.0291e-01,  1.0248e-01,  2.2938e-01,\n",
       "                        2.1558e-02, -1.7801e-02,  3.9885e-02, -1.8409e-01, -4.8231e-01,\n",
       "                       -1.0769e-01,  4.5002e-02, -2.1953e-01, -2.7714e-01,  5.4866e-03,\n",
       "                       -6.7800e-02, -8.6414e-02, -8.1131e-02],\n",
       "                      [ 1.6138e-01, -1.1203e-01,  1.9065e-02, -1.9546e-01,  1.3238e-01,\n",
       "                        4.3163e-01, -3.1089e-01, -4.0519e-01,  3.8887e-02, -5.9802e-02,\n",
       "                       -8.9400e-02, -2.7627e-02, -2.3277e-02,  6.3168e-02,  2.4665e-01,\n",
       "                       -8.6333e-02, -3.0083e-02, -6.4986e-02, -4.1686e-01,  1.0663e-01,\n",
       "                       -1.0440e-01,  1.5014e-01, -9.7877e-02,  2.6756e-01, -3.5613e-02,\n",
       "                        2.4359e-02,  3.2823e-02,  1.2372e-01, -1.1228e-03, -2.1339e-02,\n",
       "                       -3.2444e-02, -2.3022e-01,  6.6753e-02,  2.5572e-01, -2.5523e-02,\n",
       "                       -4.2646e-01,  1.0520e-01,  5.5306e-03,  1.1146e-01,  1.0509e-01,\n",
       "                        2.3484e-01,  3.1134e-02,  2.0976e-02, -7.8489e-02,  6.5610e-02,\n",
       "                        7.8607e-02, -4.9894e-02,  4.5165e-01, -1.6284e-01, -7.7122e-02,\n",
       "                       -3.7132e-02, -1.8109e-01, -1.3626e-01,  3.5982e-01,  1.0193e-01,\n",
       "                       -3.5660e-02,  2.5594e-01, -2.4535e-01, -9.9610e-02, -1.6631e-01,\n",
       "                       -1.7681e-03, -2.8263e-01, -4.1345e-02, -7.0573e-02,  2.1448e-01,\n",
       "                       -2.2894e-01,  1.0898e-01, -4.8468e-02, -1.6342e-01,  2.2899e-01,\n",
       "                       -6.0743e-02,  9.8433e-02,  2.5360e-01, -1.4911e-01,  1.4944e-01,\n",
       "                       -1.8277e-01,  2.1798e-01, -1.4786e-01, -1.6855e-01,  5.0873e-02,\n",
       "                        9.1137e-02,  5.9451e-02,  9.1195e-02, -3.3221e-01,  1.8900e-01,\n",
       "                        2.8194e-01,  2.2551e-01,  1.4986e-01,  1.2465e-01,  6.2288e-02,\n",
       "                        2.0175e-01,  1.1484e-02, -1.7204e-01,  2.9340e-01, -1.5981e-01,\n",
       "                        4.7487e-02,  8.1045e-02, -1.8826e-01, -1.5060e-01,  1.9904e-01,\n",
       "                        7.9400e-02,  3.0656e-01,  2.4975e-01,  1.0169e-01, -1.1764e-01,\n",
       "                       -1.4430e-01, -8.4629e-02,  2.9180e-02,  6.3241e-02, -1.4727e-01,\n",
       "                       -2.3047e-01,  1.4088e-01, -4.7856e-02, -6.2439e-02, -1.9667e-01,\n",
       "                        9.2213e-02, -8.0046e-02, -4.6991e-02, -2.4977e-01,  4.5436e-01,\n",
       "                       -1.5360e-01,  1.4077e-01,  2.2598e-01,  4.5249e-02,  1.1024e-01,\n",
       "                       -8.2850e-02, -8.4350e-02, -5.9918e-02],\n",
       "                      [-1.3681e-01,  8.3945e-02, -2.5553e-01,  2.1248e-01, -1.3046e-01,\n",
       "                       -2.6431e-01,  3.8153e-01, -1.1434e-01,  3.8967e-01,  2.4913e-02,\n",
       "                       -1.2922e-01,  4.9229e-02,  5.1048e-02,  1.0389e-02, -2.6515e-01,\n",
       "                        3.0827e-01, -2.1675e-01,  5.3867e-02,  2.3685e-01, -3.4153e-02,\n",
       "                       -5.7376e-02, -9.4237e-05,  7.9751e-02, -9.7266e-02,  1.2946e-01,\n",
       "                        2.3476e-01,  4.1316e-02, -1.0876e-01, -3.4086e-02, -3.3791e-01,\n",
       "                        2.9144e-01, -4.0463e-02, -2.8518e-02,  2.8201e-01, -1.6188e-01,\n",
       "                        3.9408e-01, -1.3987e-01, -3.4497e-02,  3.0106e-01, -9.1300e-02,\n",
       "                       -1.2996e-01,  5.6547e-01, -1.0818e-01,  1.9554e-01, -1.0864e-02,\n",
       "                       -1.1179e-01,  1.2059e-01, -1.5623e-01,  4.2747e-02, -3.1631e-02,\n",
       "                       -2.0868e-01, -1.3887e-01,  4.3910e-01, -2.8156e-01,  1.7766e-01,\n",
       "                        2.4664e-02, -9.0397e-02, -1.3422e-01, -5.1833e-02,  5.6965e-02,\n",
       "                       -1.4607e-02,  1.7183e-01, -1.9290e-02,  1.4617e-01,  2.0441e-02,\n",
       "                        2.4147e-01,  1.3915e-01, -7.3645e-02,  1.0773e-01, -4.3557e-02,\n",
       "                        6.9642e-03, -6.4323e-02,  2.7421e-01,  2.8666e-01, -1.3943e-01,\n",
       "                       -1.4927e-01,  3.0673e-01,  2.5270e-01,  6.1629e-02, -1.0183e-01,\n",
       "                       -4.0201e-01,  7.9714e-02, -1.0882e-01, -2.5898e-01,  1.9498e-01,\n",
       "                       -1.0367e-02,  9.8714e-02, -1.1436e-01, -1.6910e-01, -8.8843e-02,\n",
       "                       -5.0255e-02, -1.0961e-01,  1.7644e-01, -3.2788e-01,  1.4311e-01,\n",
       "                        1.2032e-01, -2.9450e-01, -2.4264e-02, -5.5023e-02,  1.7921e-01,\n",
       "                       -1.7462e-01,  2.9433e-01,  2.9585e-01,  2.6814e-02, -3.8037e-02,\n",
       "                        2.1625e-01, -1.0943e-02, -6.7808e-02,  1.7303e-01, -6.8309e-03,\n",
       "                       -1.4147e-01, -6.9097e-02,  5.3544e-02, -3.2889e-02, -5.0797e-03,\n",
       "                        9.2777e-02, -4.0568e-02,  1.4160e-01,  5.0188e-01, -2.2209e-01,\n",
       "                       -7.1699e-02, -7.9340e-02,  9.7011e-02, -5.1129e-02, -2.7430e-02,\n",
       "                        4.8924e-02,  1.2791e-01,  7.6769e-02],\n",
       "                      [-1.2079e-01, -2.3965e-02,  1.2227e-01,  5.1849e-02,  9.8901e-02,\n",
       "                        2.6513e-01, -7.2790e-02,  2.1085e-01, -6.5494e-02, -1.1568e-01,\n",
       "                       -6.4286e-02, -2.3285e-02,  4.2647e-02,  6.9333e-02,  1.5810e-01,\n",
       "                       -2.3532e-01,  1.0409e-01,  8.9550e-02,  1.0609e-01, -4.7255e-02,\n",
       "                       -4.6527e-02,  6.6797e-02, -6.3326e-02,  2.1796e-01,  1.7486e-01,\n",
       "                       -8.3323e-02,  7.2838e-02,  1.7482e-01,  1.3783e-01,  6.9136e-02,\n",
       "                       -1.1561e-01, -1.6604e-01, -2.3135e-02, -3.1117e-01,  1.6233e-01,\n",
       "                       -2.5559e-01,  2.1636e-02,  1.9247e-01,  2.8462e-02,  5.4081e-02,\n",
       "                        2.3323e-01,  9.2383e-02,  1.1100e-01, -1.1289e-01,  2.4575e-02,\n",
       "                       -1.3612e-01,  3.5052e-02, -1.3348e-01,  1.1453e-01, -3.8015e-02,\n",
       "                        1.0263e-01,  1.6047e-01, -1.9634e-01, -1.1533e-01, -3.3760e-02,\n",
       "                        1.4961e-02, -9.5764e-03,  1.4766e-01, -1.6444e-02, -1.1730e-01,\n",
       "                       -4.1854e-02, -1.5040e-01,  8.6539e-02,  1.8241e-02, -2.0354e-01,\n",
       "                       -4.7181e-01, -7.7337e-03,  2.3912e-01, -4.4859e-02, -2.4250e-01,\n",
       "                        4.6558e-01, -1.6328e-02, -3.4949e-01,  1.6429e-02,  2.0450e-01,\n",
       "                        3.7027e-01, -8.7094e-02, -1.9164e-01, -1.0471e-01,  3.9832e-03,\n",
       "                        1.7659e-01, -4.5012e-04,  1.3335e-01,  2.7352e-01, -3.1782e-01,\n",
       "                       -1.9470e-01, -2.7323e-01,  2.5580e-02, -6.1511e-02,  5.1557e-02,\n",
       "                        1.2298e-02,  2.2416e-01, -2.8610e-01,  2.0735e-02,  1.0506e-01,\n",
       "                       -2.4793e-01,  2.7823e-01,  4.8940e-01,  6.4290e-02, -3.0297e-01,\n",
       "                       -8.1950e-02, -4.1233e-01, -4.3394e-01, -3.0284e-02, -1.4721e-01,\n",
       "                       -2.6219e-01, -8.2058e-02, -4.9785e-02, -2.7402e-01, -8.7777e-02,\n",
       "                       -3.5066e-01,  6.7195e-03, -1.0329e-01, -6.8958e-02,  1.1945e-01,\n",
       "                       -2.2976e-01, -5.6242e-02,  3.9176e-02,  6.1660e-03,  3.6887e-01,\n",
       "                        2.0132e-01, -1.0244e-01,  3.6740e-02,  1.2413e-01, -2.2912e-01,\n",
       "                       -6.3398e-02,  6.2449e-02, -8.6337e-04],\n",
       "                      [-4.1378e-01,  1.1794e-01, -3.9618e-02,  9.8709e-03, -1.7257e-01,\n",
       "                       -1.6048e-03,  1.4532e-01, -9.4394e-02,  3.4631e-01,  1.0261e-01,\n",
       "                        1.0440e-01, -9.5795e-02,  8.8096e-02,  2.4075e-02, -1.4582e-01,\n",
       "                       -2.0450e-01, -8.1663e-02,  2.0291e-02, -7.8931e-02, -3.0888e-02,\n",
       "                        8.8321e-03, -1.4517e-01,  1.0698e-01, -1.9994e-01, -2.4145e-01,\n",
       "                       -4.7712e-02,  1.7620e-01, -2.6220e-01, -2.1629e-01,  3.5209e-01,\n",
       "                        6.0323e-02,  1.5676e-01, -1.1721e-02, -1.8509e-01,  3.1786e-02,\n",
       "                       -6.2276e-02,  1.4921e-02, -1.3056e-01,  7.1448e-02,  1.1468e-02,\n",
       "                       -2.1555e-01, -1.6628e-01, -9.9311e-02,  1.7438e-01,  6.2066e-02,\n",
       "                        4.5176e-02, -1.7895e-02, -1.4276e-01,  9.5229e-02,  1.1715e-03,\n",
       "                        3.0275e-02,  2.6296e-01, -1.6574e-01, -1.5815e-01, -1.8277e-01,\n",
       "                       -3.5583e-01, -1.4010e-01,  1.8569e-01, -1.0420e-01,  9.6620e-02,\n",
       "                        7.9654e-02, -1.2051e-02, -9.5900e-02, -3.5003e-02,  7.3580e-02,\n",
       "                        2.2904e-01,  1.8581e-01, -1.7667e-01,  1.4692e-01, -4.0758e-01,\n",
       "                        5.8486e-02,  1.3816e-01,  1.5126e-01, -4.9530e-02,  4.8874e-02,\n",
       "                       -1.2235e-02,  1.3209e-01, -4.8689e-01,  1.9320e-02, -3.2762e-01,\n",
       "                       -3.2507e-01,  1.0258e-01, -4.2349e-02,  2.1716e-01, -2.9775e-01,\n",
       "                        6.4865e-02, -7.1400e-02,  5.9185e-02,  1.3631e-01,  1.3769e-02,\n",
       "                       -4.2826e-02,  1.7397e-02,  3.8768e-01,  5.2994e-02,  2.6908e-01,\n",
       "                       -1.8482e-02,  1.2583e-01,  9.2937e-02,  1.1084e-01,  1.7437e-01,\n",
       "                        4.1806e-02, -2.7070e-01, -3.1483e-01,  8.6080e-02,  9.6650e-02,\n",
       "                        7.8191e-02, -5.9983e-02,  2.3116e-02, -2.3295e-01, -6.0765e-03,\n",
       "                        5.2283e-01, -1.3061e-01,  2.3645e-01, -3.0942e-03, -2.7897e-02,\n",
       "                       -1.7643e-02, -9.7149e-02, -2.8197e-02,  4.3612e-02, -6.3848e-02,\n",
       "                       -1.5127e-03, -1.0382e-01, -1.2385e-02,  1.9182e-01,  2.1288e-02,\n",
       "                       -6.2003e-02,  9.2361e-03, -9.5765e-02],\n",
       "                      [ 5.4803e-02, -1.2455e-02,  2.3139e-02,  3.0837e-03,  3.7121e-02,\n",
       "                        3.6097e-02,  7.1190e-02,  7.5727e-02, -3.6395e-02, -1.7742e-02,\n",
       "                        1.1842e-02, -7.4408e-02,  1.3814e-03, -5.0510e-02,  5.2192e-02,\n",
       "                        1.6610e-02, -7.1470e-02,  3.9244e-02, -6.9862e-02, -2.2093e-02,\n",
       "                        2.2879e-03, -5.3162e-02,  7.2135e-03, -8.4211e-02, -1.6000e-02,\n",
       "                       -7.8994e-02,  7.2046e-02,  8.8328e-02,  3.3903e-02,  4.1498e-02,\n",
       "                       -7.2547e-02,  3.8877e-02,  1.1346e-02, -6.3998e-02, -8.3069e-02,\n",
       "                       -1.5894e-02,  7.3368e-02, -7.6367e-02,  5.7791e-02, -7.2657e-02,\n",
       "                       -6.2608e-02, -8.5882e-02, -6.5535e-03,  8.3659e-02, -1.1313e-02,\n",
       "                       -6.7932e-02, -2.1986e-02, -1.5318e-02,  4.6410e-02,  3.1625e-02,\n",
       "                       -2.8270e-02,  4.8155e-02, -5.7886e-02,  5.9432e-02,  7.9956e-02,\n",
       "                        6.8619e-02,  1.5497e-02, -6.0381e-02, -5.2703e-02, -8.4363e-02,\n",
       "                       -4.4620e-02,  1.4403e-02, -1.7111e-02, -8.3287e-02,  1.1602e-02,\n",
       "                       -4.4121e-02,  4.4953e-02, -8.7624e-02,  4.0071e-02,  4.8581e-02,\n",
       "                        6.8422e-02, -2.1949e-02,  8.5563e-02,  1.3594e-02,  7.1605e-02,\n",
       "                       -6.5984e-02, -1.0768e-02, -6.3578e-02, -1.2827e-02,  7.3061e-02,\n",
       "                       -4.1877e-02,  8.6956e-02, -7.5274e-02, -2.1038e-02, -2.7472e-02,\n",
       "                        5.2082e-02,  5.6516e-02, -6.6295e-02, -5.3754e-02,  6.1799e-02,\n",
       "                        1.8283e-02, -5.5943e-02,  3.7856e-02, -2.9464e-02,  4.0538e-03,\n",
       "                       -5.8436e-04,  2.8864e-02, -8.3794e-02,  3.9023e-02,  4.6424e-02,\n",
       "                       -7.0751e-02,  4.4130e-02,  5.3943e-02,  4.1430e-02, -7.1347e-02,\n",
       "                        7.2544e-03,  8.7124e-02,  3.6050e-03,  3.2372e-02,  4.7308e-02,\n",
       "                       -2.0702e-02, -1.2615e-02,  5.8850e-02,  4.7433e-02,  1.6053e-02,\n",
       "                       -8.6907e-02,  9.8582e-03,  5.5126e-02, -7.9032e-03, -5.6815e-02,\n",
       "                       -5.7467e-02,  3.7275e-02, -4.5964e-02,  4.2242e-03, -4.2948e-02,\n",
       "                       -1.7424e-02,  3.5525e-02, -1.1963e-02],\n",
       "                      [-7.0259e-02, -7.5080e-02, -3.3871e-02,  3.6513e-02,  5.7458e-02,\n",
       "                        2.4588e-02, -1.8624e-02, -5.2335e-02,  2.6152e-02, -7.5569e-02,\n",
       "                       -1.7247e-02, -4.0634e-03, -7.9443e-02, -1.9855e-03, -3.7550e-02,\n",
       "                        2.4821e-02, -2.7345e-04,  6.4422e-02,  2.4089e-02,  6.2188e-03,\n",
       "                       -4.8822e-02,  2.9410e-02,  5.4587e-02,  4.5872e-02, -5.1888e-02,\n",
       "                       -4.9217e-02, -8.0937e-02, -1.0997e-02, -4.5452e-02,  7.1760e-03,\n",
       "                        2.1110e-03,  5.6854e-02, -6.1525e-02,  6.6278e-02,  5.1536e-03,\n",
       "                        7.2552e-02, -4.6644e-02,  2.0014e-02,  5.6858e-02,  4.6640e-03,\n",
       "                        7.6131e-02,  2.7831e-02, -8.5823e-02,  1.1690e-02, -7.6715e-02,\n",
       "                        4.4266e-02, -7.1865e-02, -6.7516e-02, -8.7224e-02, -1.8759e-02,\n",
       "                        6.8379e-02, -7.6388e-02, -7.1073e-02,  7.2800e-02,  7.4899e-02,\n",
       "                        5.0105e-02,  4.3910e-02,  7.3064e-02,  6.7230e-02, -5.7094e-03,\n",
       "                        8.7040e-02, -4.9718e-02, -3.9328e-02, -2.4864e-02,  5.5047e-02,\n",
       "                        6.0034e-02, -7.7907e-02,  2.7408e-02,  1.4623e-02,  5.2847e-02,\n",
       "                        1.3378e-02,  3.1190e-02, -2.8528e-02,  6.9548e-02, -7.0000e-02,\n",
       "                        5.9247e-02,  2.0415e-03, -3.8912e-02,  2.3725e-02,  1.6725e-02,\n",
       "                        5.7159e-02,  6.6512e-02, -6.2699e-02, -5.8854e-02,  5.0930e-02,\n",
       "                        3.2341e-04, -1.6652e-02, -8.5105e-02,  4.8426e-02, -6.8170e-03,\n",
       "                        8.2273e-02,  1.1555e-02,  8.3866e-02,  4.8577e-02, -2.6419e-02,\n",
       "                       -2.1329e-02,  2.9404e-02, -6.4056e-02, -8.0676e-02, -7.1983e-02,\n",
       "                        5.8362e-02, -9.4768e-03,  5.1168e-02, -9.6739e-03,  7.5665e-02,\n",
       "                        3.3245e-02, -4.2786e-02,  3.1908e-02, -5.5214e-02, -4.1499e-02,\n",
       "                       -3.6294e-02, -1.2828e-02,  6.0277e-02,  6.7755e-02,  7.4119e-02,\n",
       "                       -4.5059e-02,  3.3551e-02,  8.3007e-02, -5.2202e-02,  5.1572e-02,\n",
       "                        8.2690e-02, -8.2825e-02,  4.1958e-02, -1.3317e-02, -7.7278e-02,\n",
       "                        1.4942e-02,  4.2268e-02,  4.5650e-02]])),\n",
       "             ('hidden2tag.bias',\n",
       "              tensor([ 0.1511, -0.1719, -0.0899, -0.1432,  0.0524,  0.0712, -0.0426]))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'O': 0,\n",
    " 'B-MED': 1,\n",
    " 'I-MED': 2,\n",
    " 'B-rp': 3,\n",
    " 'I-rp': 4,\n",
    " '<START>': 5,\n",
    " '<STOP>': 6}\n",
    "bert_bilstm_crf.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e688e",
   "metadata": {},
   "source": [
    "## 评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5966064",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T18:20:17.959105Z",
     "start_time": "2023-04-24T18:20:17.947105Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 64*2\n",
    "EPOCHS=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3e4b724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T20:25:02.172597Z",
     "start_time": "2023-05-11T20:25:02.101605Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型加载\n",
    "bilstm = BiLSTM(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "bilstm.load_state_dict(torch.load('./Model/bilstm.pkl'))\n",
    "\n",
    "bilstm_crf = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "bilstm_crf.load_state_dict(torch.load('./Model/bilstm_crf.pkl'))\n",
    "\n",
    "bert_crf = Bert_CRF(tag_to_ix, EMBEDDING_DIM)\n",
    "bert_crf.load_state_dict(torch.load('./Model/bert_crf.pkl'))\n",
    "\n",
    "bert_att_crf = Bert_ATT_CRF(tag_to_ix, EMBEDDING_DIM)\n",
    "bert_att_crf.load_state_dict(torch.load('./Model/bert_att_crf.pkl'))\n",
    "\n",
    "bert_bilstm_crf = Bert_BiLSTM_CRF(tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "bert_bilstm_crf.load_state_dict(torch.load('./Model/bert_bilstm_crf_2.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "26c92ad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T20:27:24.054800Z",
     "start_time": "2023-05-11T20:27:22.682806Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bilstm\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6970, Recall=0.7208, F1:0.7047\n",
      "   exact mode, Precision=0.6987, Recall=0.7225, F1:0.7063\n",
      " partial mode, Precision=0.8213, Recall=0.8546, F1:0.8325\n",
      "    type mode, Precision=0.9338, Recall=0.9740, F1:0.9477\n",
      "0.7063439329220063\n",
      "Merhod\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.6980, Recall=0.7115, F1:0.6988\n",
      "   exact mode, Precision=0.6980, Recall=0.7115, F1:0.6988\n",
      " partial mode, Precision=0.8127, Recall=0.8433, F1:0.8191\n",
      "    type mode, Precision=0.9275, Recall=0.9751, F1:0.9394\n",
      "0.698781838316722\n",
      "Problem\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7193, Recall=0.7292, F1:0.7226\n",
      "   exact mode, Precision=0.7193, Recall=0.7292, F1:0.7226\n",
      " partial mode, Precision=0.8391, Recall=0.8563, F1:0.8447\n",
      "    type mode, Precision=0.9590, Recall=0.9834, F1:0.9668\n",
      "0.7225913621262459\n"
     ]
    }
   ],
   "source": [
    "print('bilstm')\n",
    "print(cal_f1(bilstm,test_data))\n",
    "print('Merhod')\n",
    "print(cal_M(bilstm,test_data))\n",
    "print('Problem')\n",
    "print(cal_P(bilstm,test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28f9e14e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T18:20:51.258069Z",
     "start_time": "2023-04-24T18:20:47.366336Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bilstm_crf\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7491, Recall=0.7529, F1:0.7474\n",
      "   exact mode, Precision=0.7507, Recall=0.7546, F1:0.7491\n",
      " partial mode, Precision=0.8590, Recall=0.8657, F1:0.8579\n",
      "    type mode, Precision=0.9615, Recall=0.9707, F1:0.9609\n",
      "0.7490903338079419\n",
      "Merhod\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7519, Recall=0.7558, F1:0.7495\n",
      "   exact mode, Precision=0.7519, Recall=0.7558, F1:0.7495\n",
      " partial mode, Precision=0.8499, Recall=0.8571, F1:0.8477\n",
      "    type mode, Precision=0.9480, Recall=0.9585, F1:0.9460\n",
      "0.749501661129568\n",
      "Problem\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7276, Recall=0.7359, F1:0.7303\n",
      "   exact mode, Precision=0.7276, Recall=0.7359, F1:0.7303\n",
      " partial mode, Precision=0.8475, Recall=0.8596, F1:0.8513\n",
      "    type mode, Precision=0.9673, Recall=0.9834, F1:0.9723\n",
      "0.730343300110742\n"
     ]
    }
   ],
   "source": [
    "print('bilstm_crf')\n",
    "print(cal_f1(bilstm_crf))\n",
    "print('Merhod')\n",
    "print(cal_M(bilstm_crf))\n",
    "print('Problem')\n",
    "print(cal_P(bilstm_crf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b2bf5ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T18:22:06.945015Z",
     "start_time": "2023-04-24T18:20:57.314080Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_crf\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7865, Recall=0.7816, F1:0.7780\n",
      "   exact mode, Precision=0.7949, Recall=0.7877, F1:0.7848\n",
      " partial mode, Precision=0.8805, Recall=0.8717, F1:0.8685\n",
      "    type mode, Precision=0.9557, Recall=0.9468, F1:0.9430\n",
      "0.7848204398038288\n",
      "Merhod\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7503, Recall=0.7614, F1:0.7484\n",
      "   exact mode, Precision=0.7503, Recall=0.7614, F1:0.7484\n",
      " partial mode, Precision=0.8391, Recall=0.8533, F1:0.8372\n",
      "    type mode, Precision=0.9280, Recall=0.9452, F1:0.9260\n",
      "0.7483942414174969\n",
      "Problem\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7924, Recall=0.7973, F1:0.7940\n",
      "   exact mode, Precision=0.7924, Recall=0.7973, F1:0.7940\n",
      " partial mode, Precision=0.8679, Recall=0.8754, F1:0.8704\n",
      "    type mode, Precision=0.9435, Recall=0.9535, F1:0.9468\n",
      "0.7940199335548173\n"
     ]
    }
   ],
   "source": [
    "print('bert_crf')\n",
    "print(bert_cal_F1(bert_crf))\n",
    "print('Merhod')\n",
    "print(bert_cal_M(bert_crf))\n",
    "print('Problem')\n",
    "print(bert_cal_P(bert_crf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "66397e5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T18:23:16.235573Z",
     "start_time": "2023-04-24T18:22:06.947015Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_att_crf\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7688, Recall=0.7763, F1:0.7672\n",
      "   exact mode, Precision=0.7782, Recall=0.7841, F1:0.7755\n",
      " partial mode, Precision=0.8671, Recall=0.8753, F1:0.8646\n",
      "    type mode, Precision=0.9421, Recall=0.9527, F1:0.9403\n",
      "0.7754627432368298\n",
      "Merhod\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7381, Recall=0.7553, F1:0.7398\n",
      "   exact mode, Precision=0.7381, Recall=0.7553, F1:0.7398\n",
      " partial mode, Precision=0.8372, Recall=0.8596, F1:0.8398\n",
      "    type mode, Precision=0.9363, Recall=0.9640, F1:0.9398\n",
      "0.7397563676633441\n",
      "Problem\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7791, Recall=0.7940, F1:0.7841\n",
      "   exact mode, Precision=0.7791, Recall=0.7940, F1:0.7841\n",
      " partial mode, Precision=0.8530, Recall=0.8704, F1:0.8588\n",
      "    type mode, Precision=0.9269, Recall=0.9468, F1:0.9336\n",
      "0.7840531561461793\n"
     ]
    }
   ],
   "source": [
    "print('bert_att_crf')\n",
    "print(bert_cal_F1(bert_att_crf))\n",
    "print('Merhod')\n",
    "print(bert_cal_M(bert_att_crf))\n",
    "print('Problem')\n",
    "print(bert_cal_P(bert_att_crf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aea420fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T18:24:26.956805Z",
     "start_time": "2023-04-24T18:23:16.236571Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_bilstm_crf\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.8209, Recall=0.8148, F1:0.8144\n",
      "   exact mode, Precision=0.8319, Recall=0.8225, F1:0.8232\n",
      " partial mode, Precision=0.9055, Recall=0.8958, F1:0.8962\n",
      "    type mode, Precision=0.9679, Recall=0.9612, F1:0.9604\n",
      "0.8232320835310871\n",
      "Merhod\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.8023, Recall=0.7996, F1:0.7953\n",
      "   exact mode, Precision=0.8023, Recall=0.7996, F1:0.7953\n",
      " partial mode, Precision=0.8832, Recall=0.8832, F1:0.8771\n",
      "    type mode, Precision=0.9640, Recall=0.9668, F1:0.9589\n",
      "0.795348837209302\n",
      "Problem\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.8272, Recall=0.8306, F1:0.8283\n",
      "   exact mode, Precision=0.8272, Recall=0.8306, F1:0.8283\n",
      " partial mode, Precision=0.8962, Recall=0.8995, F1:0.8970\n",
      "    type mode, Precision=0.9651, Recall=0.9684, F1:0.9657\n",
      "0.8283499446290145\n"
     ]
    }
   ],
   "source": [
    "print('bert_bilstm_crf')\n",
    "print(bert_cal_F1(bert_bilstm_crf))\n",
    "print('Merhod')\n",
    "print(bert_cal_M(bert_bilstm_crf))\n",
    "print('Problem')\n",
    "print(bert_cal_P(bert_bilstm_crf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23563bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(15),F)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(np.arange(15),train_acc_lst3,label=\"train_acc\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.title('Accuracy')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
